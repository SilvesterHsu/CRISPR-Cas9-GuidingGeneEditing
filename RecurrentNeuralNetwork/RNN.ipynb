{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for filename in glob.glob(\"*.tsv\"):\n",
    "    df = pd.read_csv(filename, sep=\"\\t\")\n",
    "    li.append(df)\n",
    "    \n",
    "data = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"danRer11_chopchop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Target sequence</th>\n",
       "      <th>Genomic location</th>\n",
       "      <th>Self-complementarity</th>\n",
       "      <th>MM0</th>\n",
       "      <th>MM1</th>\n",
       "      <th>MM2</th>\n",
       "      <th>MM3</th>\n",
       "      <th>Efficiency</th>\n",
       "      <th>Batch Zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CGATGTTGGGAAACTTGGGTAGG</td>\n",
       "      <td>chr10:10016472</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.94</td>\n",
       "      <td>chr10:10016197-10016497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GATGAATGAGGAACGCGCAGCGG</td>\n",
       "      <td>chr10:10016403</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.36</td>\n",
       "      <td>chr10:10016197-10016497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>TCAATTCATTATTCACGCGGAGG</td>\n",
       "      <td>chr10:10016437</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59.94</td>\n",
       "      <td>chr10:10016197-10016497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ACGCGTCTTGAGCACTCGCTGGG</td>\n",
       "      <td>chr10:10016211</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.73</td>\n",
       "      <td>chr10:10016197-10016497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>AAGATCGATGTTGGGAAACTTGG</td>\n",
       "      <td>chr10:10016477</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.63</td>\n",
       "      <td>chr10:10016197-10016497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283021</th>\n",
       "      <td>17</td>\n",
       "      <td>CAGAAAAGAAGATACTCTGGGGG</td>\n",
       "      <td>chr9:9961117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>70.45</td>\n",
       "      <td>chr9:9960819-9961119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283022</th>\n",
       "      <td>18</td>\n",
       "      <td>CTGGGGGAAAAAAAGCTGAAAGG</td>\n",
       "      <td>chr9:9961101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>58.27</td>\n",
       "      <td>chr9:9960819-9961119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283023</th>\n",
       "      <td>19</td>\n",
       "      <td>GAGCAGAAAAGAAGATACTCTGG</td>\n",
       "      <td>chr9:9961120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>40.29</td>\n",
       "      <td>chr9:9960819-9961119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283024</th>\n",
       "      <td>20</td>\n",
       "      <td>GCAGAAAAGAAGATACTCTGGGG</td>\n",
       "      <td>chr9:9961118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>69.60</td>\n",
       "      <td>chr9:9960819-9961119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283025</th>\n",
       "      <td>21</td>\n",
       "      <td>AGTATTTGTTTTTTCTGCTTTGG</td>\n",
       "      <td>chr9:9961051</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>180</td>\n",
       "      <td>&gt;=136</td>\n",
       "      <td>33.06</td>\n",
       "      <td>chr9:9960819-9961119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283026 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Rank          Target sequence Genomic location  Self-complementarity  \\\n",
       "0          1  CGATGTTGGGAAACTTGGGTAGG   chr10:10016472                     0   \n",
       "1          2  GATGAATGAGGAACGCGCAGCGG   chr10:10016403                     0   \n",
       "2          3  TCAATTCATTATTCACGCGGAGG   chr10:10016437                     0   \n",
       "3          4  ACGCGTCTTGAGCACTCGCTGGG   chr10:10016211                     0   \n",
       "4          5  AAGATCGATGTTGGGAAACTTGG   chr10:10016477                     0   \n",
       "...      ...                      ...              ...                   ...   \n",
       "283021    17  CAGAAAAGAAGATACTCTGGGGG     chr9:9961117                     0   \n",
       "283022    18  CTGGGGGAAAAAAAGCTGAAAGG     chr9:9961101                     0   \n",
       "283023    19  GAGCAGAAAAGAAGATACTCTGG     chr9:9961120                     1   \n",
       "283024    20  GCAGAAAAGAAGATACTCTGGGG     chr9:9961118                     0   \n",
       "283025    21  AGTATTTGTTTTTTCTGCTTTGG     chr9:9961051                     0   \n",
       "\n",
       "        MM0  MM1  MM2    MM3  Efficiency               Batch Zone  \n",
       "0         1    0    0      0       70.94  chr10:10016197-10016497  \n",
       "1         1    0    0      0       64.36  chr10:10016197-10016497  \n",
       "2         1    0    0      0       59.94  chr10:10016197-10016497  \n",
       "3         1    0    0      0       51.73  chr10:10016197-10016497  \n",
       "4         1    0    0      0       38.63  chr10:10016197-10016497  \n",
       "...     ...  ...  ...    ...         ...                      ...  \n",
       "283021    0    0    4     29       70.45     chr9:9960819-9961119  \n",
       "283022    0    0    5     32       58.27     chr9:9960819-9961119  \n",
       "283023    0    0    3     38       40.29     chr9:9960819-9961119  \n",
       "283024    0    0    2     47       69.60     chr9:9960819-9961119  \n",
       "283025    1    7  180  >=136       33.06     chr9:9960819-9961119  \n",
       "\n",
       "[283026 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:10000]\n",
    "test_data = data[10000:12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTGCGTAATCTTGTGAGAGTCGG'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Target sequence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       59.78\n",
       "1       38.36\n",
       "2       58.56\n",
       "3       62.58\n",
       "4       60.03\n",
       "        ...  \n",
       "1182    48.42\n",
       "1183    29.75\n",
       "1184    52.38\n",
       "1185    38.71\n",
       "1186    46.64\n",
       "Name: Efficiency, Length: 1187, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Efficiency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_sequence(data['Target sequence'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sequence(seq):\n",
    "    m = np.zeros((len(seq), 4))\n",
    "    for i, char in enumerate(seq):\n",
    "        if char == 'A':\n",
    "            m[i][0] = 1\n",
    "        elif char == 'T':\n",
    "            m[i][1] = 1\n",
    "        elif char == 'C':\n",
    "            m[i][2] = 1\n",
    "        elif char == 'G':\n",
    "            m[i][3] = 1\n",
    "    m = m.reshape(m.shape[0]*m.shape[1])\n",
    "    return m\n",
    "\n",
    "def transform_sequence_rnn(seq):\n",
    "    m = np.zeros((len(seq), 4))\n",
    "    for i, char in enumerate(seq):\n",
    "        if char == 'A':\n",
    "            m[i][0] = 1\n",
    "        elif char == 'T':\n",
    "            m[i][1] = 1\n",
    "        elif char == 'C':\n",
    "            m[i][2] = 1\n",
    "        elif char == 'G':\n",
    "            m[i][3] = 1\n",
    "#     m = m.reshape(m.shape[0]*m.shape[1])\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TTGCGTAATCTTGTGAGAGTCGG', 'CACACATACGTCCGTGCTGCTGG',\n",
       "       'GCTCCCTCTAGTGCTTTGGTTGG', ..., 'CATTGAGAGCCGTGTGCCGAAGG',\n",
       "       'ACACGGCTCTCAATGACATTTGG', 'TCAAAACTTTTTCCTATGAAGGG'], dtype=object)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Target sequence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_sequence(data['Target sequence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDataset(object):\n",
    "    def __init__(self, data, use_rnn=False):\n",
    "        self.target_sequence = data['Target sequence'].values\n",
    "        self.efficiency = data['Efficiency'].values\n",
    "        self.use_rnn = use_rnn\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.use_rnn:\n",
    "            seq = torch.as_tensor(transform_sequence_rnn(self.target_sequence[idx]), dtype=torch.float32)\n",
    "        else:\n",
    "            seq = torch.as_tensor(transform_sequence(self.target_sequence[idx]), dtype=torch.float32)\n",
    "        eff = torch.as_tensor(self.efficiency[idx] / 100, dtype=torch.float32)\n",
    "        return seq, eff\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(92, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class RNN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN_Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=4, hidden_size=16, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, _ = self.lstm(x)\n",
    "        x2 = torch.mean(x1, 1)\n",
    "#         x3 = self.fc(x1[:,-1,:])\n",
    "        x3 = torch.sigmoid(x2)\n",
    "        \n",
    "        return x3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "seq_len = 23\n",
    "input_dim = 4\n",
    "n_layers = 1\n",
    "hidden_dim = 16\n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)\n",
    "\n",
    "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = lstm_layer(inp, hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 23, 16])\n",
      "torch.Size([1, 64, 16])\n",
      "torch.Size([64, 16])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)\n",
    "print(hidden[0].shape)\n",
    "print(torch.mean(out, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNN_Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = GeneDataset(train_data, use_rnn=True)\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ,loss: 2.514204996638\n",
      "Epoch 1 ,loss: 2.5159356743097305\n",
      "Epoch 2 ,loss: 2.4934799736365676\n",
      "Epoch 3 ,loss: 2.483655429445207\n",
      "Epoch 4 ,loss: 2.469254986383021\n",
      "Epoch 5 ,loss: 2.4467031359672546\n",
      "Epoch 6 ,loss: 2.4049555994570255\n",
      "Epoch 7 ,loss: 2.3525924663990736\n",
      "Epoch 8 ,loss: 2.2975352043285966\n",
      "Epoch 9 ,loss: 2.2633271254599094\n",
      "Epoch 10 ,loss: 2.2532203029841185\n",
      "Epoch 11 ,loss: 2.2462245505303144\n",
      "Epoch 12 ,loss: 2.235874073114246\n",
      "Epoch 13 ,loss: 2.2284156708046794\n",
      "Epoch 14 ,loss: 2.214628697372973\n",
      "Epoch 15 ,loss: 2.207522557582706\n",
      "Epoch 16 ,loss: 2.2054627994075418\n",
      "Epoch 17 ,loss: 2.2013190928846598\n",
      "Epoch 18 ,loss: 2.18560887966305\n",
      "Epoch 19 ,loss: 2.1852404293604195\n",
      "Epoch 20 ,loss: 2.1796050556004047\n",
      "Epoch 21 ,loss: 2.1731291571632028\n",
      "Epoch 22 ,loss: 2.168731168843806\n",
      "Epoch 23 ,loss: 2.163150707259774\n",
      "Epoch 24 ,loss: 2.1564439618960023\n",
      "Epoch 25 ,loss: 2.148312389384955\n",
      "Epoch 26 ,loss: 2.137854639440775\n",
      "Epoch 27 ,loss: 2.131053335033357\n",
      "Epoch 28 ,loss: 2.1258258083835244\n",
      "Epoch 29 ,loss: 2.1046613063663244\n",
      "Epoch 30 ,loss: 2.091652658767998\n",
      "Epoch 31 ,loss: 2.0719383391551673\n",
      "Epoch 32 ,loss: 2.0505975959822536\n",
      "Epoch 33 ,loss: 2.023893976584077\n",
      "Epoch 34 ,loss: 1.9965927172452211\n",
      "Epoch 35 ,loss: 1.9856059942394495\n",
      "Epoch 36 ,loss: 1.9651583773083985\n",
      "Epoch 37 ,loss: 1.9437211654148996\n",
      "Epoch 38 ,loss: 1.9204234057106078\n",
      "Epoch 39 ,loss: 1.8994752829894423\n",
      "Epoch 40 ,loss: 1.8810414345934987\n",
      "Epoch 41 ,loss: 1.8780266866087914\n",
      "Epoch 42 ,loss: 1.8673975253477693\n",
      "Epoch 43 ,loss: 1.8488817634060979\n",
      "Epoch 44 ,loss: 1.8449785276316106\n",
      "Epoch 45 ,loss: 1.8345512594096363\n",
      "Epoch 46 ,loss: 1.825724312569946\n",
      "Epoch 47 ,loss: 1.8254037559963763\n",
      "Epoch 48 ,loss: 1.8168466840870678\n",
      "Epoch 49 ,loss: 1.8017240960616618\n",
      "Epoch 50 ,loss: 1.80603340966627\n",
      "Epoch 51 ,loss: 1.7903001154772937\n",
      "Epoch 52 ,loss: 1.7938559101894498\n",
      "Epoch 53 ,loss: 1.7888359683565795\n",
      "Epoch 54 ,loss: 1.7931061531417072\n",
      "Epoch 55 ,loss: 1.7825071825645864\n",
      "Epoch 56 ,loss: 1.777259735390544\n",
      "Epoch 57 ,loss: 1.7722476366907358\n",
      "Epoch 58 ,loss: 1.7730016955174506\n",
      "Epoch 59 ,loss: 1.7667268053628504\n",
      "Epoch 60 ,loss: 1.7603960083797574\n",
      "Epoch 61 ,loss: 1.7567870886996388\n",
      "Epoch 62 ,loss: 1.757396258879453\n",
      "Epoch 63 ,loss: 1.7573092430830002\n",
      "Epoch 64 ,loss: 1.7526535671204329\n",
      "Epoch 65 ,loss: 1.7529078009538352\n",
      "Epoch 66 ,loss: 1.7542404648847878\n",
      "Epoch 67 ,loss: 1.751147573813796\n",
      "Epoch 68 ,loss: 1.7409217837266624\n",
      "Epoch 69 ,loss: 1.7351078297942877\n",
      "Epoch 70 ,loss: 1.7386424811556935\n",
      "Epoch 71 ,loss: 1.7250015293247998\n",
      "Epoch 72 ,loss: 1.723290992435068\n",
      "Epoch 73 ,loss: 1.7319919355213642\n",
      "Epoch 74 ,loss: 1.7250792151317\n",
      "Epoch 75 ,loss: 1.7170069497078657\n",
      "Epoch 76 ,loss: 1.7168673565611243\n",
      "Epoch 77 ,loss: 1.7106118630617857\n",
      "Epoch 78 ,loss: 1.7119120405986905\n",
      "Epoch 79 ,loss: 1.7078115912154317\n",
      "Epoch 80 ,loss: 1.7143879793584347\n",
      "Epoch 81 ,loss: 1.7026664237491786\n",
      "Epoch 82 ,loss: 1.7042867075651884\n",
      "Epoch 83 ,loss: 1.6958810528740287\n",
      "Epoch 84 ,loss: 1.696360332891345\n",
      "Epoch 85 ,loss: 1.69702501129359\n",
      "Epoch 86 ,loss: 1.6881155625451356\n",
      "Epoch 87 ,loss: 1.683215360622853\n",
      "Epoch 88 ,loss: 1.687328228726983\n",
      "Epoch 89 ,loss: 1.6808179523795843\n",
      "Epoch 90 ,loss: 1.6790579436346889\n",
      "Epoch 91 ,loss: 1.6775138517841697\n",
      "Epoch 92 ,loss: 1.6721546375192702\n",
      "Epoch 93 ,loss: 1.6843787916004658\n",
      "Epoch 94 ,loss: 1.6778214392252266\n",
      "Epoch 95 ,loss: 1.6671857303008437\n",
      "Epoch 96 ,loss: 1.6723876232281327\n",
      "Epoch 97 ,loss: 1.6674680528230965\n",
      "Epoch 98 ,loss: 1.6609882973134518\n",
      "Epoch 99 ,loss: 1.6617068308405578\n",
      "Epoch 100 ,loss: 1.6577282617799938\n",
      "Epoch 101 ,loss: 1.6494808648712933\n",
      "Epoch 102 ,loss: 1.6506178611889482\n",
      "Epoch 103 ,loss: 1.6520662405528128\n",
      "Epoch 104 ,loss: 1.6472474983893335\n",
      "Epoch 105 ,loss: 1.6468738736584783\n",
      "Epoch 106 ,loss: 1.6436364890541881\n",
      "Epoch 107 ,loss: 1.6406338033266366\n",
      "Epoch 108 ,loss: 1.641812895424664\n",
      "Epoch 109 ,loss: 1.6412747418507934\n",
      "Epoch 110 ,loss: 1.636420494876802\n",
      "Epoch 111 ,loss: 1.6325976802036166\n",
      "Epoch 112 ,loss: 1.6344305728562176\n",
      "Epoch 113 ,loss: 1.6298255124129355\n",
      "Epoch 114 ,loss: 1.6329635991714895\n",
      "Epoch 115 ,loss: 1.6291936927009374\n",
      "Epoch 116 ,loss: 1.6340426588431\n",
      "Epoch 117 ,loss: 1.6181267350912094\n",
      "Epoch 118 ,loss: 1.6252037603408098\n",
      "Epoch 119 ,loss: 1.6194622926414013\n",
      "Epoch 120 ,loss: 1.6130864373408258\n",
      "Epoch 121 ,loss: 1.6065903506241739\n",
      "Epoch 122 ,loss: 1.605083366855979\n",
      "Epoch 123 ,loss: 1.6099598044529557\n",
      "Epoch 124 ,loss: 1.603059919551015\n",
      "Epoch 125 ,loss: 1.6036448604427278\n",
      "Epoch 126 ,loss: 1.5935110612772405\n",
      "Epoch 127 ,loss: 1.5868574711494148\n",
      "Epoch 128 ,loss: 1.5830508866347373\n",
      "Epoch 129 ,loss: 1.584297416266054\n",
      "Epoch 130 ,loss: 1.5769372372888029\n",
      "Epoch 131 ,loss: 1.5783279812894762\n",
      "Epoch 132 ,loss: 1.5729422918520868\n",
      "Epoch 133 ,loss: 1.5709068668074906\n",
      "Epoch 134 ,loss: 1.5607314594089985\n",
      "Epoch 135 ,loss: 1.558979846071452\n",
      "Epoch 136 ,loss: 1.5529419565573335\n",
      "Epoch 137 ,loss: 1.5560027910396457\n",
      "Epoch 138 ,loss: 1.5420194012112916\n",
      "Epoch 139 ,loss: 1.5439672437496483\n",
      "Epoch 140 ,loss: 1.5361145385541022\n",
      "Epoch 141 ,loss: 1.5297104506753385\n",
      "Epoch 142 ,loss: 1.5214861161075532\n",
      "Epoch 143 ,loss: 1.516480655875057\n",
      "Epoch 144 ,loss: 1.5287458975799382\n",
      "Epoch 145 ,loss: 1.5276209725998342\n",
      "Epoch 146 ,loss: 1.5161056900396943\n",
      "Epoch 147 ,loss: 1.5025656521320343\n",
      "Epoch 148 ,loss: 1.5044865156523883\n",
      "Epoch 149 ,loss: 1.4990721363574266\n",
      "Epoch 150 ,loss: 1.494802011642605\n",
      "Epoch 151 ,loss: 1.4967021853663027\n",
      "Epoch 152 ,loss: 1.5032304949127138\n",
      "Epoch 153 ,loss: 1.4860322293825448\n",
      "Epoch 154 ,loss: 1.4782466632314026\n",
      "Epoch 155 ,loss: 1.4830185971222818\n",
      "Epoch 156 ,loss: 1.4817163716070354\n",
      "Epoch 157 ,loss: 1.4738906165584922\n",
      "Epoch 158 ,loss: 1.4716038620099425\n",
      "Epoch 159 ,loss: 1.469075292814523\n",
      "Epoch 160 ,loss: 1.4696566187776625\n",
      "Epoch 161 ,loss: 1.4599216999486089\n",
      "Epoch 162 ,loss: 1.459482720354572\n",
      "Epoch 163 ,loss: 1.4710647631436586\n",
      "Epoch 164 ,loss: 1.4512284453958273\n",
      "Epoch 165 ,loss: 1.4605524418875575\n",
      "Epoch 166 ,loss: 1.4519064300693572\n",
      "Epoch 167 ,loss: 1.4575723400339484\n",
      "Epoch 168 ,loss: 1.4551361752673984\n",
      "Epoch 169 ,loss: 1.4498634808696806\n",
      "Epoch 170 ,loss: 1.4438343453221023\n",
      "Epoch 171 ,loss: 1.4431057660840452\n",
      "Epoch 172 ,loss: 1.4489417341537774\n",
      "Epoch 173 ,loss: 1.4508785596117377\n",
      "Epoch 174 ,loss: 1.4281071238219738\n",
      "Epoch 175 ,loss: 1.4341049818322062\n",
      "Epoch 176 ,loss: 1.4367315266281366\n",
      "Epoch 177 ,loss: 1.44103715242818\n",
      "Epoch 178 ,loss: 1.4301993353292346\n",
      "Epoch 179 ,loss: 1.4257960636168718\n",
      "Epoch 180 ,loss: 1.4274857905693352\n",
      "Epoch 181 ,loss: 1.4248506296426058\n",
      "Epoch 182 ,loss: 1.428914153482765\n",
      "Epoch 183 ,loss: 1.4259981559589505\n",
      "Epoch 184 ,loss: 1.4183972785249352\n",
      "Epoch 185 ,loss: 1.4255389953032136\n",
      "Epoch 186 ,loss: 1.4202255099080503\n",
      "Epoch 187 ,loss: 1.4182439246214926\n",
      "Epoch 188 ,loss: 1.4080109279602766\n",
      "Epoch 189 ,loss: 1.415848492179066\n",
      "Epoch 190 ,loss: 1.4101192681118846\n",
      "Epoch 191 ,loss: 1.4043244100175798\n",
      "Epoch 192 ,loss: 1.4041256187483668\n",
      "Epoch 193 ,loss: 1.4127731793560088\n",
      "Epoch 194 ,loss: 1.4032399822026491\n",
      "Epoch 195 ,loss: 1.4067746615037322\n",
      "Epoch 196 ,loss: 1.4101914302445948\n",
      "Epoch 197 ,loss: 1.4033266543410718\n",
      "Epoch 198 ,loss: 1.3943011034280062\n",
      "Epoch 199 ,loss: 1.3990016370080411\n",
      "Epoch 200 ,loss: 1.4000851749442518\n",
      "Epoch 201 ,loss: 1.3968205540440977\n",
      "Epoch 202 ,loss: 1.4024125952273607\n",
      "Epoch 203 ,loss: 1.393081362824887\n",
      "Epoch 204 ,loss: 1.3962946911342442\n",
      "Epoch 205 ,loss: 1.397655036766082\n",
      "Epoch 206 ,loss: 1.3957643462345004\n",
      "Epoch 207 ,loss: 1.38112265849486\n",
      "Epoch 208 ,loss: 1.3860696624033153\n",
      "Epoch 209 ,loss: 1.3857652116566896\n",
      "Epoch 210 ,loss: 1.3861680487170815\n",
      "Epoch 211 ,loss: 1.3824240174144506\n",
      "Epoch 212 ,loss: 1.3788209185004234\n",
      "Epoch 213 ,loss: 1.382870802655816\n",
      "Epoch 214 ,loss: 1.3752343324013054\n",
      "Epoch 215 ,loss: 1.3870747066102922\n",
      "Epoch 216 ,loss: 1.390599171165377\n",
      "Epoch 217 ,loss: 1.3798573948442936\n",
      "Epoch 218 ,loss: 1.3773640124127269\n",
      "Epoch 219 ,loss: 1.380197600927204\n",
      "Epoch 220 ,loss: 1.3708047410473228\n",
      "Epoch 221 ,loss: 1.3744804807938635\n",
      "Epoch 222 ,loss: 1.3698305827565491\n",
      "Epoch 223 ,loss: 1.375821459107101\n",
      "Epoch 224 ,loss: 1.3705515014007688\n",
      "Epoch 225 ,loss: 1.3716729767620564\n",
      "Epoch 226 ,loss: 1.363014868926257\n",
      "Epoch 227 ,loss: 1.370209793560207\n",
      "Epoch 228 ,loss: 1.366568258497864\n",
      "Epoch 229 ,loss: 1.3803161908872426\n",
      "Epoch 230 ,loss: 1.37614353466779\n",
      "Epoch 231 ,loss: 1.362411874346435\n",
      "Epoch 232 ,loss: 1.365806468296796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233 ,loss: 1.3702361327596009\n",
      "Epoch 234 ,loss: 1.3595450734719634\n",
      "Epoch 235 ,loss: 1.3573345611803234\n",
      "Epoch 236 ,loss: 1.367452866397798\n",
      "Epoch 237 ,loss: 1.3708941009826958\n",
      "Epoch 238 ,loss: 1.362440211698413\n",
      "Epoch 239 ,loss: 1.35294488677755\n",
      "Epoch 240 ,loss: 1.3540351730771363\n",
      "Epoch 241 ,loss: 1.3600806728936732\n",
      "Epoch 242 ,loss: 1.3615943198092282\n",
      "Epoch 243 ,loss: 1.3510226835496724\n",
      "Epoch 244 ,loss: 1.3513199109584093\n",
      "Epoch 245 ,loss: 1.3497275495901704\n",
      "Epoch 246 ,loss: 1.347585535608232\n",
      "Epoch 247 ,loss: 1.3486528075300157\n",
      "Epoch 248 ,loss: 1.3488802025094628\n",
      "Epoch 249 ,loss: 1.3420351026579738\n",
      "Epoch 250 ,loss: 1.3387743416242301\n",
      "Epoch 251 ,loss: 1.3439036458730698\n",
      "Epoch 252 ,loss: 1.3495812439359725\n",
      "Epoch 253 ,loss: 1.3390296828001738\n",
      "Epoch 254 ,loss: 1.3507283218204975\n",
      "Epoch 255 ,loss: 1.3418721375055611\n",
      "Epoch 256 ,loss: 1.335901897167787\n",
      "Epoch 257 ,loss: 1.3375207991339266\n",
      "Epoch 258 ,loss: 1.3407345805317163\n",
      "Epoch 259 ,loss: 1.330197413917631\n",
      "Epoch 260 ,loss: 1.3404656858183444\n",
      "Epoch 261 ,loss: 1.3384100431576371\n",
      "Epoch 262 ,loss: 1.331134743988514\n",
      "Epoch 263 ,loss: 1.331455226521939\n",
      "Epoch 264 ,loss: 1.337210568599403\n",
      "Epoch 265 ,loss: 1.3339003389701247\n",
      "Epoch 266 ,loss: 1.3377871694974601\n",
      "Epoch 267 ,loss: 1.3319822861813009\n",
      "Epoch 268 ,loss: 1.3317584209144115\n",
      "Epoch 269 ,loss: 1.3268322106450796\n",
      "Epoch 270 ,loss: 1.335079476237297\n",
      "Epoch 271 ,loss: 1.346634202171117\n",
      "Epoch 272 ,loss: 1.3278877860866487\n",
      "Epoch 273 ,loss: 1.3265344514511526\n",
      "Epoch 274 ,loss: 1.3287137211300433\n",
      "Epoch 275 ,loss: 1.3287346493452787\n",
      "Epoch 276 ,loss: 1.3222511783242226\n",
      "Epoch 277 ,loss: 1.3239238988608122\n",
      "Epoch 278 ,loss: 1.3211960820481181\n",
      "Epoch 279 ,loss: 1.3191589880734682\n",
      "Epoch 280 ,loss: 1.3245302275754511\n",
      "Epoch 281 ,loss: 1.3176526492461562\n",
      "Epoch 282 ,loss: 1.3198244189843535\n",
      "Epoch 283 ,loss: 1.3179750852286816\n",
      "Epoch 284 ,loss: 1.3168084807693958\n",
      "Epoch 285 ,loss: 1.3152787061408162\n",
      "Epoch 286 ,loss: 1.3196249580942094\n",
      "Epoch 287 ,loss: 1.3128419388085604\n",
      "Epoch 288 ,loss: 1.3181545194238424\n",
      "Epoch 289 ,loss: 1.3152754483744502\n",
      "Epoch 290 ,loss: 1.31624258402735\n",
      "Epoch 291 ,loss: 1.3105435050092638\n",
      "Epoch 292 ,loss: 1.3119431934319437\n",
      "Epoch 293 ,loss: 1.312656876631081\n",
      "Epoch 294 ,loss: 1.3070923248305917\n",
      "Epoch 295 ,loss: 1.314477093052119\n",
      "Epoch 296 ,loss: 1.3104993808083236\n",
      "Epoch 297 ,loss: 1.3134790915064514\n",
      "Epoch 298 ,loss: 1.3078720108605921\n",
      "Epoch 299 ,loss: 1.3068870403803885\n",
      "Epoch 300 ,loss: 1.304119094274938\n",
      "Epoch 301 ,loss: 1.3081051036715508\n",
      "Epoch 302 ,loss: 1.3017722116783261\n",
      "Epoch 303 ,loss: 1.3073171582072973\n",
      "Epoch 304 ,loss: 1.300363848451525\n",
      "Epoch 305 ,loss: 1.3048856863752007\n",
      "Epoch 306 ,loss: 1.2957621803507209\n",
      "Epoch 307 ,loss: 1.3002559943124652\n",
      "Epoch 308 ,loss: 1.295279547572136\n",
      "Epoch 309 ,loss: 1.3043747963383794\n",
      "Epoch 310 ,loss: 1.3055879017338157\n",
      "Epoch 311 ,loss: 1.305604939814657\n",
      "Epoch 312 ,loss: 1.3129629213362932\n",
      "Epoch 313 ,loss: 1.3042516470886767\n",
      "Epoch 314 ,loss: 1.2987979697063565\n",
      "Epoch 315 ,loss: 1.2944179084151983\n",
      "Epoch 316 ,loss: 1.3031688574701548\n",
      "Epoch 317 ,loss: 1.2983667235821486\n",
      "Epoch 318 ,loss: 1.2973347730003297\n",
      "Epoch 319 ,loss: 1.297130549326539\n",
      "Epoch 320 ,loss: 1.2916339416988194\n",
      "Epoch 321 ,loss: 1.2908248086459935\n",
      "Epoch 322 ,loss: 1.2958951187320054\n",
      "Epoch 323 ,loss: 1.2900801161304116\n",
      "Epoch 324 ,loss: 1.2926155049353838\n",
      "Epoch 325 ,loss: 1.295096572022885\n",
      "Epoch 326 ,loss: 1.2839857409708202\n",
      "Epoch 327 ,loss: 1.2995170638896525\n",
      "Epoch 328 ,loss: 1.285860457457602\n",
      "Epoch 329 ,loss: 1.2829633899964392\n",
      "Epoch 330 ,loss: 1.296694023301825\n",
      "Epoch 331 ,loss: 1.2839217400178313\n",
      "Epoch 332 ,loss: 1.2904860600829124\n",
      "Epoch 333 ,loss: 1.2943809889256954\n",
      "Epoch 334 ,loss: 1.2853441857732832\n",
      "Epoch 335 ,loss: 1.2882244877982885\n",
      "Epoch 336 ,loss: 1.2910483898594975\n",
      "Epoch 337 ,loss: 1.2877414929680526\n",
      "Epoch 338 ,loss: 1.2796515228692442\n",
      "Epoch 339 ,loss: 1.2787299011833966\n",
      "Epoch 340 ,loss: 1.283221687655896\n",
      "Epoch 341 ,loss: 1.2844335748814046\n",
      "Epoch 342 ,loss: 1.281800917815417\n",
      "Epoch 343 ,loss: 1.280416373629123\n",
      "Epoch 344 ,loss: 1.278582140803337\n",
      "Epoch 345 ,loss: 1.2978362333960831\n",
      "Epoch 346 ,loss: 1.2780159935355186\n",
      "Epoch 347 ,loss: 1.2874656445346773\n",
      "Epoch 348 ,loss: 1.2802683552727103\n",
      "Epoch 349 ,loss: 1.2814427143894136\n",
      "Epoch 350 ,loss: 1.2774808667600155\n",
      "Epoch 351 ,loss: 1.2807348347268999\n",
      "Epoch 352 ,loss: 1.2887781579047441\n",
      "Epoch 353 ,loss: 1.2770948740653694\n",
      "Epoch 354 ,loss: 1.283047906588763\n",
      "Epoch 355 ,loss: 1.2809179951436818\n",
      "Epoch 356 ,loss: 1.2808564533479512\n",
      "Epoch 357 ,loss: 1.28140251012519\n",
      "Epoch 358 ,loss: 1.2801964320242405\n",
      "Epoch 359 ,loss: 1.2778794700279832\n",
      "Epoch 360 ,loss: 1.2783479411154985\n",
      "Epoch 361 ,loss: 1.2773438696749508\n",
      "Epoch 362 ,loss: 1.2714922190643847\n",
      "Epoch 363 ,loss: 1.271556071471423\n",
      "Epoch 364 ,loss: 1.266691381111741\n",
      "Epoch 365 ,loss: 1.2686361046507955\n",
      "Epoch 366 ,loss: 1.272802134975791\n",
      "Epoch 367 ,loss: 1.2681180769577622\n",
      "Epoch 368 ,loss: 1.2746878657490015\n",
      "Epoch 369 ,loss: 1.2773002260364592\n",
      "Epoch 370 ,loss: 1.2679709368385375\n",
      "Epoch 371 ,loss: 1.2678801203146577\n",
      "Epoch 372 ,loss: 1.276964524295181\n",
      "Epoch 373 ,loss: 1.2648503135424107\n",
      "Epoch 374 ,loss: 1.2655408536083996\n",
      "Epoch 375 ,loss: 1.2801983910612762\n",
      "Epoch 376 ,loss: 1.2706234068609774\n",
      "Epoch 377 ,loss: 1.2651598900556564\n",
      "Epoch 378 ,loss: 1.2679751459509134\n",
      "Epoch 379 ,loss: 1.2734426776878536\n",
      "Epoch 380 ,loss: 1.2661469290032983\n",
      "Epoch 381 ,loss: 1.270203051622957\n",
      "Epoch 382 ,loss: 1.2682079649530351\n",
      "Epoch 383 ,loss: 1.2683930932544172\n",
      "Epoch 384 ,loss: 1.2591644735075533\n",
      "Epoch 385 ,loss: 1.2697384310886264\n",
      "Epoch 386 ,loss: 1.2679598173126578\n",
      "Epoch 387 ,loss: 1.2621904239058495\n",
      "Epoch 388 ,loss: 1.2616523425094783\n",
      "Epoch 389 ,loss: 1.2637800057418644\n",
      "Epoch 390 ,loss: 1.2615017257630825\n",
      "Epoch 391 ,loss: 1.2607663488015532\n",
      "Epoch 392 ,loss: 1.2656718119978905\n",
      "Epoch 393 ,loss: 1.258576539810747\n",
      "Epoch 394 ,loss: 1.267797568347305\n",
      "Epoch 395 ,loss: 1.2593816169537604\n",
      "Epoch 396 ,loss: 1.2654856266453862\n",
      "Epoch 397 ,loss: 1.2655886174179614\n",
      "Epoch 398 ,loss: 1.260316246189177\n",
      "Epoch 399 ,loss: 1.2600216050632298\n",
      "Epoch 400 ,loss: 1.2624006578698754\n",
      "Epoch 401 ,loss: 1.256180356722325\n",
      "Epoch 402 ,loss: 1.2649137820117176\n",
      "Epoch 403 ,loss: 1.2660866477526724\n",
      "Epoch 404 ,loss: 1.2624998493120074\n",
      "Epoch 405 ,loss: 1.2590098632499576\n",
      "Epoch 406 ,loss: 1.26166568743065\n",
      "Epoch 407 ,loss: 1.2628996958956122\n",
      "Epoch 408 ,loss: 1.2513583642430604\n",
      "Epoch 409 ,loss: 1.2536031152121723\n",
      "Epoch 410 ,loss: 1.2596660014241934\n",
      "Epoch 411 ,loss: 1.258887602481991\n",
      "Epoch 412 ,loss: 1.267012479249388\n",
      "Epoch 413 ,loss: 1.2526395143941045\n",
      "Epoch 414 ,loss: 1.2610040940344334\n",
      "Epoch 415 ,loss: 1.24894230812788\n",
      "Epoch 416 ,loss: 1.2548288963735104\n",
      "Epoch 417 ,loss: 1.2652412876486778\n",
      "Epoch 418 ,loss: 1.2585399658419192\n",
      "Epoch 419 ,loss: 1.2530577625147998\n",
      "Epoch 420 ,loss: 1.247442635241896\n",
      "Epoch 421 ,loss: 1.2607509554363787\n",
      "Epoch 422 ,loss: 1.2498649307526648\n",
      "Epoch 423 ,loss: 1.2560822595842183\n",
      "Epoch 424 ,loss: 1.2541201366111636\n",
      "Epoch 425 ,loss: 1.2510805374477059\n",
      "Epoch 426 ,loss: 1.2488726465962827\n",
      "Epoch 427 ,loss: 1.2524823788553476\n",
      "Epoch 428 ,loss: 1.2596605657599866\n",
      "Epoch 429 ,loss: 1.2558313719928265\n",
      "Epoch 430 ,loss: 1.2604353493079543\n",
      "Epoch 431 ,loss: 1.2625348246656358\n",
      "Epoch 432 ,loss: 1.2470460925251245\n",
      "Epoch 433 ,loss: 1.2529340791516006\n",
      "Epoch 434 ,loss: 1.2566486652940512\n",
      "Epoch 435 ,loss: 1.2564062410965562\n",
      "Epoch 436 ,loss: 1.2483550920151174\n",
      "Epoch 437 ,loss: 1.2454361845739186\n",
      "Epoch 438 ,loss: 1.252574993763119\n",
      "Epoch 439 ,loss: 1.2522471654228866\n",
      "Epoch 440 ,loss: 1.2486122441478074\n",
      "Epoch 441 ,loss: 1.2494600866921246\n",
      "Epoch 442 ,loss: 1.2508856821805239\n",
      "Epoch 443 ,loss: 1.2571800728328526\n",
      "Epoch 444 ,loss: 1.2461785548366606\n",
      "Epoch 445 ,loss: 1.2542628645896912\n",
      "Epoch 446 ,loss: 1.2493145451880991\n",
      "Epoch 447 ,loss: 1.2490782658569515\n",
      "Epoch 448 ,loss: 1.2447898634709418\n",
      "Epoch 449 ,loss: 1.250459180213511\n",
      "Epoch 450 ,loss: 1.2459680018946528\n",
      "Epoch 451 ,loss: 1.2432377594523132\n",
      "Epoch 452 ,loss: 1.250468319747597\n",
      "Epoch 453 ,loss: 1.2388639613054693\n",
      "Epoch 454 ,loss: 1.2440597601234913\n",
      "Epoch 455 ,loss: 1.2448480390012264\n",
      "Epoch 456 ,loss: 1.2412229580804706\n",
      "Epoch 457 ,loss: 1.2420600862242281\n",
      "Epoch 458 ,loss: 1.2457493054680526\n",
      "Epoch 459 ,loss: 1.2426119428128004\n",
      "Epoch 460 ,loss: 1.252003876492381\n",
      "Epoch 461 ,loss: 1.244116755668074\n",
      "Epoch 462 ,loss: 1.2447011624462903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 463 ,loss: 1.2377893798984587\n",
      "Epoch 464 ,loss: 1.2420594117138535\n",
      "Epoch 465 ,loss: 1.2404697756282985\n",
      "Epoch 466 ,loss: 1.2376735066063702\n",
      "Epoch 467 ,loss: 1.2424041708000004\n",
      "Epoch 468 ,loss: 1.2484887572936714\n",
      "Epoch 469 ,loss: 1.2358752149157226\n",
      "Epoch 470 ,loss: 1.2465306425001472\n",
      "Epoch 471 ,loss: 1.23953393753618\n",
      "Epoch 472 ,loss: 1.2388776787556708\n",
      "Epoch 473 ,loss: 1.241737405769527\n",
      "Epoch 474 ,loss: 1.2456393893808126\n",
      "Epoch 475 ,loss: 1.2455051974393427\n",
      "Epoch 476 ,loss: 1.2398031037300825\n",
      "Epoch 477 ,loss: 1.2410291456617415\n",
      "Epoch 478 ,loss: 1.2395324958488345\n",
      "Epoch 479 ,loss: 1.2406102470122278\n",
      "Epoch 480 ,loss: 1.2377249686978757\n",
      "Epoch 481 ,loss: 1.2376517378725111\n",
      "Epoch 482 ,loss: 1.2346416041254997\n",
      "Epoch 483 ,loss: 1.2343202116899192\n",
      "Epoch 484 ,loss: 1.2339724595658481\n",
      "Epoch 485 ,loss: 1.2374164503999054\n",
      "Epoch 486 ,loss: 1.2300389853771776\n",
      "Epoch 487 ,loss: 1.2345951315946877\n",
      "Epoch 488 ,loss: 1.2390905641950667\n",
      "Epoch 489 ,loss: 1.2311498397029936\n",
      "Epoch 490 ,loss: 1.2334413072094321\n",
      "Epoch 491 ,loss: 1.2400013231672347\n",
      "Epoch 492 ,loss: 1.2311348468065262\n",
      "Epoch 493 ,loss: 1.2352568181231618\n",
      "Epoch 494 ,loss: 1.234916892601177\n",
      "Epoch 495 ,loss: 1.2364231240935624\n",
      "Epoch 496 ,loss: 1.2296964572742581\n",
      "Epoch 497 ,loss: 1.2336120554246008\n",
      "Epoch 498 ,loss: 1.23404322238639\n",
      "Epoch 499 ,loss: 1.2310779844410717\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, ele in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        seq, eff = ele\n",
    "#         print(ele)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(seq)\n",
    "#         print(outputs.shape)\n",
    "#         print(\"outputs\", outputs, \"eff\", eff)\n",
    "        loss = criterion(outputs[:, 0], eff)\n",
    "#         print(\"loss\", loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch {} ,loss: {}\".format(epoch, running_loss))\n",
    "    \n",
    "print('Finished Training')\n",
    "# train_time[8] = time.time() - now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = GeneDataset(test_data, use_rnn=True)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=1,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs tensor([0.4250], grad_fn=<SelectBackward>) eff tensor([0.4625])\n",
      "outputs tensor([0.5037], grad_fn=<SelectBackward>) eff tensor([0.4376])\n",
      "outputs tensor([0.5765], grad_fn=<SelectBackward>) eff tensor([0.6153])\n",
      "outputs tensor([0.5796], grad_fn=<SelectBackward>) eff tensor([0.6121])\n",
      "outputs tensor([0.2934], grad_fn=<SelectBackward>) eff tensor([0.2319])\n",
      "outputs tensor([0.5205], grad_fn=<SelectBackward>) eff tensor([0.5630])\n",
      "outputs tensor([0.5796], grad_fn=<SelectBackward>) eff tensor([0.5140])\n",
      "outputs tensor([0.4618], grad_fn=<SelectBackward>) eff tensor([0.4843])\n",
      "outputs tensor([0.6120], grad_fn=<SelectBackward>) eff tensor([0.6284])\n",
      "outputs tensor([0.4306], grad_fn=<SelectBackward>) eff tensor([0.4739])\n",
      "outputs tensor([0.6549], grad_fn=<SelectBackward>) eff tensor([0.5728])\n",
      "outputs tensor([0.4092], grad_fn=<SelectBackward>) eff tensor([0.3839])\n",
      "outputs tensor([0.4084], grad_fn=<SelectBackward>) eff tensor([0.4373])\n",
      "outputs tensor([0.3175], grad_fn=<SelectBackward>) eff tensor([0.3250])\n",
      "outputs tensor([0.6284], grad_fn=<SelectBackward>) eff tensor([0.6538])\n",
      "outputs tensor([0.5802], grad_fn=<SelectBackward>) eff tensor([0.5085])\n",
      "outputs tensor([0.4294], grad_fn=<SelectBackward>) eff tensor([0.4014])\n",
      "outputs tensor([0.5863], grad_fn=<SelectBackward>) eff tensor([0.5136])\n",
      "outputs tensor([0.5743], grad_fn=<SelectBackward>) eff tensor([0.6277])\n",
      "outputs tensor([0.6686], grad_fn=<SelectBackward>) eff tensor([0.5877])\n",
      "outputs tensor([0.5241], grad_fn=<SelectBackward>) eff tensor([0.5806])\n",
      "outputs tensor([0.3093], grad_fn=<SelectBackward>) eff tensor([0.3556])\n",
      "outputs tensor([0.6155], grad_fn=<SelectBackward>) eff tensor([0.6639])\n",
      "outputs tensor([0.4735], grad_fn=<SelectBackward>) eff tensor([0.4312])\n",
      "outputs tensor([0.6146], grad_fn=<SelectBackward>) eff tensor([0.6992])\n",
      "outputs tensor([0.5009], grad_fn=<SelectBackward>) eff tensor([0.5578])\n",
      "outputs tensor([0.5815], grad_fn=<SelectBackward>) eff tensor([0.6085])\n",
      "outputs tensor([0.5727], grad_fn=<SelectBackward>) eff tensor([0.5946])\n",
      "outputs tensor([0.4770], grad_fn=<SelectBackward>) eff tensor([0.5471])\n",
      "outputs tensor([0.4290], grad_fn=<SelectBackward>) eff tensor([0.4972])\n",
      "outputs tensor([0.5995], grad_fn=<SelectBackward>) eff tensor([0.6736])\n",
      "outputs tensor([0.3863], grad_fn=<SelectBackward>) eff tensor([0.4767])\n",
      "outputs tensor([0.5767], grad_fn=<SelectBackward>) eff tensor([0.5597])\n",
      "outputs tensor([0.4806], grad_fn=<SelectBackward>) eff tensor([0.4363])\n",
      "outputs tensor([0.4492], grad_fn=<SelectBackward>) eff tensor([0.4611])\n",
      "outputs tensor([0.4924], grad_fn=<SelectBackward>) eff tensor([0.4889])\n",
      "outputs tensor([0.5421], grad_fn=<SelectBackward>) eff tensor([0.4502])\n",
      "outputs tensor([0.5662], grad_fn=<SelectBackward>) eff tensor([0.6359])\n",
      "outputs tensor([0.5827], grad_fn=<SelectBackward>) eff tensor([0.5479])\n",
      "outputs tensor([0.6614], grad_fn=<SelectBackward>) eff tensor([0.6265])\n",
      "outputs tensor([0.3839], grad_fn=<SelectBackward>) eff tensor([0.3513])\n",
      "outputs tensor([0.5032], grad_fn=<SelectBackward>) eff tensor([0.5381])\n",
      "outputs tensor([0.5957], grad_fn=<SelectBackward>) eff tensor([0.5707])\n",
      "outputs tensor([0.3980], grad_fn=<SelectBackward>) eff tensor([0.4783])\n",
      "outputs tensor([0.3180], grad_fn=<SelectBackward>) eff tensor([0.3382])\n",
      "outputs tensor([0.5792], grad_fn=<SelectBackward>) eff tensor([0.6607])\n",
      "outputs tensor([0.5924], grad_fn=<SelectBackward>) eff tensor([0.5678])\n",
      "outputs tensor([0.5680], grad_fn=<SelectBackward>) eff tensor([0.5450])\n",
      "outputs tensor([0.5082], grad_fn=<SelectBackward>) eff tensor([0.4967])\n",
      "outputs tensor([0.6495], grad_fn=<SelectBackward>) eff tensor([0.6551])\n",
      "outputs tensor([0.7227], grad_fn=<SelectBackward>) eff tensor([0.8048])\n",
      "outputs tensor([0.5611], grad_fn=<SelectBackward>) eff tensor([0.5646])\n",
      "outputs tensor([0.4967], grad_fn=<SelectBackward>) eff tensor([0.5943])\n",
      "outputs tensor([0.6591], grad_fn=<SelectBackward>) eff tensor([0.6729])\n",
      "outputs tensor([0.4700], grad_fn=<SelectBackward>) eff tensor([0.4296])\n",
      "outputs tensor([0.4541], grad_fn=<SelectBackward>) eff tensor([0.4064])\n",
      "outputs tensor([0.6006], grad_fn=<SelectBackward>) eff tensor([0.6139])\n",
      "outputs tensor([0.4682], grad_fn=<SelectBackward>) eff tensor([0.4873])\n",
      "outputs tensor([0.4352], grad_fn=<SelectBackward>) eff tensor([0.5153])\n",
      "outputs tensor([0.5024], grad_fn=<SelectBackward>) eff tensor([0.4648])\n",
      "outputs tensor([0.5245], grad_fn=<SelectBackward>) eff tensor([0.4963])\n",
      "outputs tensor([0.5936], grad_fn=<SelectBackward>) eff tensor([0.6783])\n",
      "outputs tensor([0.5662], grad_fn=<SelectBackward>) eff tensor([0.4363])\n",
      "outputs tensor([0.6067], grad_fn=<SelectBackward>) eff tensor([0.6818])\n",
      "outputs tensor([0.4300], grad_fn=<SelectBackward>) eff tensor([0.4294])\n",
      "outputs tensor([0.3438], grad_fn=<SelectBackward>) eff tensor([0.3467])\n",
      "outputs tensor([0.2365], grad_fn=<SelectBackward>) eff tensor([0.2318])\n",
      "outputs tensor([0.3503], grad_fn=<SelectBackward>) eff tensor([0.4156])\n",
      "outputs tensor([0.6196], grad_fn=<SelectBackward>) eff tensor([0.6243])\n",
      "outputs tensor([0.3280], grad_fn=<SelectBackward>) eff tensor([0.3562])\n",
      "outputs tensor([0.3528], grad_fn=<SelectBackward>) eff tensor([0.3108])\n",
      "outputs tensor([0.5867], grad_fn=<SelectBackward>) eff tensor([0.5842])\n",
      "outputs tensor([0.4393], grad_fn=<SelectBackward>) eff tensor([0.4908])\n",
      "outputs tensor([0.5724], grad_fn=<SelectBackward>) eff tensor([0.5589])\n",
      "outputs tensor([0.3859], grad_fn=<SelectBackward>) eff tensor([0.3979])\n",
      "outputs tensor([0.5372], grad_fn=<SelectBackward>) eff tensor([0.5308])\n",
      "outputs tensor([0.6582], grad_fn=<SelectBackward>) eff tensor([0.7177])\n",
      "outputs tensor([0.5294], grad_fn=<SelectBackward>) eff tensor([0.6795])\n",
      "outputs tensor([0.5633], grad_fn=<SelectBackward>) eff tensor([0.5908])\n",
      "outputs tensor([0.3201], grad_fn=<SelectBackward>) eff tensor([0.3697])\n",
      "outputs tensor([0.2457], grad_fn=<SelectBackward>) eff tensor([0.3384])\n",
      "outputs tensor([0.3933], grad_fn=<SelectBackward>) eff tensor([0.3920])\n",
      "outputs tensor([0.4498], grad_fn=<SelectBackward>) eff tensor([0.4322])\n",
      "outputs tensor([0.2173], grad_fn=<SelectBackward>) eff tensor([0.2441])\n",
      "outputs tensor([0.4405], grad_fn=<SelectBackward>) eff tensor([0.4141])\n",
      "outputs tensor([0.4315], grad_fn=<SelectBackward>) eff tensor([0.3435])\n",
      "outputs tensor([0.4585], grad_fn=<SelectBackward>) eff tensor([0.4132])\n",
      "outputs tensor([0.4586], grad_fn=<SelectBackward>) eff tensor([0.3421])\n",
      "outputs tensor([0.2419], grad_fn=<SelectBackward>) eff tensor([0.2812])\n",
      "outputs tensor([0.3331], grad_fn=<SelectBackward>) eff tensor([0.3557])\n",
      "outputs tensor([0.3360], grad_fn=<SelectBackward>) eff tensor([0.2884])\n",
      "outputs tensor([0.3992], grad_fn=<SelectBackward>) eff tensor([0.4983])\n",
      "outputs tensor([0.5928], grad_fn=<SelectBackward>) eff tensor([0.5872])\n",
      "outputs tensor([0.6061], grad_fn=<SelectBackward>) eff tensor([0.5871])\n",
      "outputs tensor([0.6633], grad_fn=<SelectBackward>) eff tensor([0.5589])\n",
      "outputs tensor([0.6722], grad_fn=<SelectBackward>) eff tensor([0.6481])\n",
      "outputs tensor([0.4523], grad_fn=<SelectBackward>) eff tensor([0.4501])\n",
      "outputs tensor([0.4577], grad_fn=<SelectBackward>) eff tensor([0.5103])\n",
      "outputs tensor([0.6978], grad_fn=<SelectBackward>) eff tensor([0.7572])\n",
      "outputs tensor([0.3169], grad_fn=<SelectBackward>) eff tensor([0.4251])\n",
      "outputs tensor([0.5093], grad_fn=<SelectBackward>) eff tensor([0.5875])\n",
      "outputs tensor([0.3923], grad_fn=<SelectBackward>) eff tensor([0.3101])\n",
      "outputs tensor([0.6480], grad_fn=<SelectBackward>) eff tensor([0.6430])\n",
      "outputs tensor([0.3894], grad_fn=<SelectBackward>) eff tensor([0.3572])\n",
      "outputs tensor([0.5062], grad_fn=<SelectBackward>) eff tensor([0.5460])\n",
      "outputs tensor([0.5389], grad_fn=<SelectBackward>) eff tensor([0.5577])\n",
      "outputs tensor([0.5328], grad_fn=<SelectBackward>) eff tensor([0.5747])\n",
      "outputs tensor([0.5547], grad_fn=<SelectBackward>) eff tensor([0.5499])\n",
      "outputs tensor([0.4482], grad_fn=<SelectBackward>) eff tensor([0.4123])\n",
      "outputs tensor([0.4736], grad_fn=<SelectBackward>) eff tensor([0.5022])\n",
      "outputs tensor([0.5452], grad_fn=<SelectBackward>) eff tensor([0.6938])\n",
      "outputs tensor([0.6389], grad_fn=<SelectBackward>) eff tensor([0.6985])\n",
      "outputs tensor([0.6028], grad_fn=<SelectBackward>) eff tensor([0.6507])\n",
      "outputs tensor([0.5620], grad_fn=<SelectBackward>) eff tensor([0.5655])\n",
      "outputs tensor([0.6046], grad_fn=<SelectBackward>) eff tensor([0.5723])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs tensor([0.1862], grad_fn=<SelectBackward>) eff tensor([0.0127])\n",
      "outputs tensor([0.6725], grad_fn=<SelectBackward>) eff tensor([0.6679])\n",
      "outputs tensor([0.5476], grad_fn=<SelectBackward>) eff tensor([0.4851])\n",
      "outputs tensor([0.4032], grad_fn=<SelectBackward>) eff tensor([0.4302])\n",
      "outputs tensor([0.6320], grad_fn=<SelectBackward>) eff tensor([0.6480])\n",
      "outputs tensor([0.5234], grad_fn=<SelectBackward>) eff tensor([0.3951])\n",
      "outputs tensor([0.5181], grad_fn=<SelectBackward>) eff tensor([0.5296])\n",
      "outputs tensor([0.5841], grad_fn=<SelectBackward>) eff tensor([0.6425])\n",
      "outputs tensor([0.4498], grad_fn=<SelectBackward>) eff tensor([0.4452])\n",
      "outputs tensor([0.5400], grad_fn=<SelectBackward>) eff tensor([0.5736])\n",
      "outputs tensor([0.6234], grad_fn=<SelectBackward>) eff tensor([0.6283])\n",
      "outputs tensor([0.1193], grad_fn=<SelectBackward>) eff tensor([0.2149])\n",
      "outputs tensor([0.6640], grad_fn=<SelectBackward>) eff tensor([0.6067])\n",
      "outputs tensor([0.6764], grad_fn=<SelectBackward>) eff tensor([0.7257])\n",
      "outputs tensor([0.5330], grad_fn=<SelectBackward>) eff tensor([0.5417])\n",
      "outputs tensor([0.3504], grad_fn=<SelectBackward>) eff tensor([0.3784])\n",
      "outputs tensor([0.4327], grad_fn=<SelectBackward>) eff tensor([0.4393])\n",
      "outputs tensor([0.5379], grad_fn=<SelectBackward>) eff tensor([0.4831])\n",
      "outputs tensor([0.5853], grad_fn=<SelectBackward>) eff tensor([0.6358])\n",
      "outputs tensor([0.5475], grad_fn=<SelectBackward>) eff tensor([0.6272])\n",
      "outputs tensor([0.5791], grad_fn=<SelectBackward>) eff tensor([0.6001])\n",
      "outputs tensor([0.4449], grad_fn=<SelectBackward>) eff tensor([0.4782])\n",
      "outputs tensor([0.4568], grad_fn=<SelectBackward>) eff tensor([0.4999])\n",
      "outputs tensor([0.7095], grad_fn=<SelectBackward>) eff tensor([0.7437])\n",
      "outputs tensor([0.3515], grad_fn=<SelectBackward>) eff tensor([0.4324])\n",
      "outputs tensor([0.5789], grad_fn=<SelectBackward>) eff tensor([0.5774])\n",
      "outputs tensor([0.5914], grad_fn=<SelectBackward>) eff tensor([0.6387])\n",
      "outputs tensor([0.4338], grad_fn=<SelectBackward>) eff tensor([0.4369])\n",
      "outputs tensor([0.3671], grad_fn=<SelectBackward>) eff tensor([0.3195])\n",
      "outputs tensor([0.5594], grad_fn=<SelectBackward>) eff tensor([0.5999])\n",
      "outputs tensor([0.5488], grad_fn=<SelectBackward>) eff tensor([0.4889])\n",
      "outputs tensor([0.3956], grad_fn=<SelectBackward>) eff tensor([0.2995])\n",
      "outputs tensor([0.2227], grad_fn=<SelectBackward>) eff tensor([0.2722])\n",
      "outputs tensor([0.6037], grad_fn=<SelectBackward>) eff tensor([0.6137])\n",
      "outputs tensor([0.5942], grad_fn=<SelectBackward>) eff tensor([0.5649])\n",
      "outputs tensor([0.6076], grad_fn=<SelectBackward>) eff tensor([0.5338])\n",
      "outputs tensor([0.4105], grad_fn=<SelectBackward>) eff tensor([0.3920])\n",
      "outputs tensor([0.4603], grad_fn=<SelectBackward>) eff tensor([0.4831])\n",
      "outputs tensor([0.5664], grad_fn=<SelectBackward>) eff tensor([0.6491])\n",
      "outputs tensor([0.7007], grad_fn=<SelectBackward>) eff tensor([0.7019])\n",
      "outputs tensor([0.3663], grad_fn=<SelectBackward>) eff tensor([0.3247])\n",
      "outputs tensor([0.2589], grad_fn=<SelectBackward>) eff tensor([0.1551])\n",
      "outputs tensor([0.4337], grad_fn=<SelectBackward>) eff tensor([0.4105])\n",
      "outputs tensor([0.2940], grad_fn=<SelectBackward>) eff tensor([0.2580])\n",
      "outputs tensor([0.6977], grad_fn=<SelectBackward>) eff tensor([0.7685])\n",
      "outputs tensor([0.3939], grad_fn=<SelectBackward>) eff tensor([0.3513])\n",
      "outputs tensor([0.5964], grad_fn=<SelectBackward>) eff tensor([0.6021])\n",
      "outputs tensor([0.5069], grad_fn=<SelectBackward>) eff tensor([0.6174])\n",
      "outputs tensor([0.4435], grad_fn=<SelectBackward>) eff tensor([0.5769])\n",
      "outputs tensor([0.6862], grad_fn=<SelectBackward>) eff tensor([0.6595])\n",
      "outputs tensor([0.5650], grad_fn=<SelectBackward>) eff tensor([0.5369])\n",
      "outputs tensor([0.2783], grad_fn=<SelectBackward>) eff tensor([0.3016])\n",
      "outputs tensor([0.6076], grad_fn=<SelectBackward>) eff tensor([0.6218])\n",
      "outputs tensor([0.5987], grad_fn=<SelectBackward>) eff tensor([0.5522])\n",
      "outputs tensor([0.7001], grad_fn=<SelectBackward>) eff tensor([0.6560])\n",
      "outputs tensor([0.4585], grad_fn=<SelectBackward>) eff tensor([0.4891])\n",
      "outputs tensor([0.5466], grad_fn=<SelectBackward>) eff tensor([0.5623])\n",
      "outputs tensor([0.6357], grad_fn=<SelectBackward>) eff tensor([0.5769])\n",
      "outputs tensor([0.4038], grad_fn=<SelectBackward>) eff tensor([0.4527])\n",
      "outputs tensor([0.5466], grad_fn=<SelectBackward>) eff tensor([0.5448])\n",
      "outputs tensor([0.6818], grad_fn=<SelectBackward>) eff tensor([0.7244])\n",
      "outputs tensor([0.4649], grad_fn=<SelectBackward>) eff tensor([0.4709])\n",
      "outputs tensor([0.2856], grad_fn=<SelectBackward>) eff tensor([0.3228])\n",
      "outputs tensor([0.5670], grad_fn=<SelectBackward>) eff tensor([0.5127])\n",
      "outputs tensor([0.2252], grad_fn=<SelectBackward>) eff tensor([0.1867])\n",
      "outputs tensor([0.3261], grad_fn=<SelectBackward>) eff tensor([0.4366])\n",
      "outputs tensor([0.3583], grad_fn=<SelectBackward>) eff tensor([0.4129])\n",
      "outputs tensor([0.6360], grad_fn=<SelectBackward>) eff tensor([0.6921])\n",
      "outputs tensor([0.5176], grad_fn=<SelectBackward>) eff tensor([0.5031])\n",
      "outputs tensor([0.5359], grad_fn=<SelectBackward>) eff tensor([0.4253])\n",
      "outputs tensor([0.6100], grad_fn=<SelectBackward>) eff tensor([0.6027])\n",
      "outputs tensor([0.3471], grad_fn=<SelectBackward>) eff tensor([0.3679])\n",
      "outputs tensor([0.1655], grad_fn=<SelectBackward>) eff tensor([0.1030])\n",
      "outputs tensor([0.5168], grad_fn=<SelectBackward>) eff tensor([0.4504])\n",
      "outputs tensor([0.5503], grad_fn=<SelectBackward>) eff tensor([0.4973])\n",
      "outputs tensor([0.4616], grad_fn=<SelectBackward>) eff tensor([0.4624])\n",
      "outputs tensor([0.3256], grad_fn=<SelectBackward>) eff tensor([0.3128])\n",
      "outputs tensor([0.4888], grad_fn=<SelectBackward>) eff tensor([0.4523])\n",
      "outputs tensor([0.6016], grad_fn=<SelectBackward>) eff tensor([0.5954])\n",
      "outputs tensor([0.3063], grad_fn=<SelectBackward>) eff tensor([0.3926])\n",
      "outputs tensor([0.5670], grad_fn=<SelectBackward>) eff tensor([0.5638])\n",
      "outputs tensor([0.6942], grad_fn=<SelectBackward>) eff tensor([0.6089])\n",
      "outputs tensor([0.6591], grad_fn=<SelectBackward>) eff tensor([0.5745])\n",
      "outputs tensor([0.4359], grad_fn=<SelectBackward>) eff tensor([0.4241])\n",
      "outputs tensor([0.5153], grad_fn=<SelectBackward>) eff tensor([0.5395])\n",
      "tensor(0.0032, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mse = 0\n",
    "for i, ele in enumerate(testloader):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    seq, eff = ele\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(seq)\n",
    "    if i % 10 == 0:\n",
    "        print(\"outputs\", outputs[0], \"eff\", eff)\n",
    "    mse += criterion(outputs[0], eff)\n",
    "mse = mse / len(test_set)\n",
    "print(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.05673287\n"
     ]
    }
   ],
   "source": [
    "RMSE = np.sqrt(mse.detach().numpy())\n",
    "print(\"RMSE:\", RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
