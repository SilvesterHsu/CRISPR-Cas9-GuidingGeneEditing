{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T17:29:07.130954Z",
     "start_time": "2020-04-15T17:29:07.123649Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T17:29:09.220427Z",
     "start_time": "2020-04-15T17:29:08.964432Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/danrer11_chopchop_train.csv',index_col=0)\n",
    "data_test = pd.read_csv('data/danrer11_chopchop_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data\n",
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T17:29:14.953865Z",
     "start_time": "2020-04-15T17:29:10.500645Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: (226420, 4, 23)\n",
      "Test dataset size: (56606, 4, 23)\n"
     ]
    }
   ],
   "source": [
    "encoding = {'A':np.array([1,0,0,0]),\n",
    "            'C':np.array([0,1,0,0]),\n",
    "            'G':np.array([0,0,1,0]),\n",
    "            'T':np.array([0,0,0,1])}\n",
    "\n",
    "def one_hot(guide,encoding):\n",
    "    data = np.zeros((4,len(guide)))\n",
    "    assert data.shape == (4,23)\n",
    "    for i in range(data.shape[-1]):\n",
    "        data[:,i] = encoding[guide[i]]\n",
    "    return data\n",
    "\n",
    "#print(one_hot('CTGATCACGGCTGAAGGACTCGG',encoding))\n",
    "\n",
    "def batch_one_hot(data,encoding):\n",
    "    guides = np.zeros((len(data),4,23))\n",
    "    i=0\n",
    "    for guide in data['GUIDE']:\n",
    "        guides[i] = one_hot(guide,encoding)\n",
    "        i+=1\n",
    "    return guides\n",
    "\n",
    "guides_train = batch_one_hot(data_train,encoding)\n",
    "guides_test = batch_one_hot(data_test,encoding)\n",
    "print('Train dataset size:',guides_train.shape)\n",
    "print('Test dataset size:',guides_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot('CTGATCACGGCTGAAGGACTCGG',encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Pytorch` data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T17:30:21.903809Z",
     "start_time": "2020-04-15T17:30:21.875715Z"
    }
   },
   "outputs": [],
   "source": [
    "class GGEDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample.float()\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "GGE_dataset_train = GGEDataset(data = guides_train, transform = transform)\n",
    "GGE_dataset_test = GGEDataset(data = guides_test, transform = transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(GGE_dataset_train, batch_size=50000,shuffle=False, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(GGE_dataset_test, batch_size=50000,shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226420"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(GGE_dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T17:30:32.433205Z",
     "start_time": "2020-04-15T17:30:32.421931Z"
    }
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(92, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 30)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(30, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 92),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 2)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.ConvTranspose2d(6, 1, 2)\n",
    "        self.pool2 =nn.MaxUnpool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,1*4*23)\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1,1,4,23)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T17:30:50.862891Z",
     "start_time": "2020-04-15T17:30:50.856729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T17:30:54.766467Z",
     "start_time": "2020-04-15T17:30:54.744482Z"
    }
   },
   "outputs": [],
   "source": [
    "net = Autoencoder().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T17:30:58.747124Z",
     "start_time": "2020-04-15T17:30:58.738679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('model/autoencoder.pth.tar')\n",
    "net.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Logistic model on chopchop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer all the guide from 92 dimension to 30 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(GGE_dataset_train, batch_size=len(GGE_dataset_train),shuffle=False, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(GGE_dataset_test, batch_size=len(GGE_dataset_test),shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([226420, 30])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 13.5784,  -4.2706,  -2.8988,  ...,   6.9587, -11.0836,  -8.5389],\n",
       "        [ 13.8553,   2.9260,  -2.9646,  ...,   1.1597,  -3.6170,   4.3032],\n",
       "        [  4.2005,  -1.9159,  -1.7123,  ...,   5.3553, -10.7488,  -9.3842],\n",
       "        ...,\n",
       "        [  9.8289,   9.3090,  -5.1207,  ...,   9.4204,  -9.4076,  -0.6384],\n",
       "        [  2.7749,   4.6147, -10.3113,  ...,   3.9229,  -4.0526,   1.1860],\n",
       "        [ -2.3375,  -1.3460,  -5.7669,  ...,  -0.0944,  -8.8136,   8.5558]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        tr_fea = net.encoder(data.view(-1,1*4*23).to(device)).to('cpu')\n",
    "        # break\n",
    "print(\"Data shape: {}\".format(tr_fea.shape))\n",
    "tr_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([56606, 30])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        test_fea = net.encoder(data.view(-1,1*4*23).to(device)).to('cpu')\n",
    "print(\"Data shape: {}\".format(test_fea.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDataset(object):\n",
    "    def __init__(self, guide, eff):\n",
    "        self.target_sequence = guide\n",
    "        self.efficiency = eff.values\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.target_sequence[idx]\n",
    "        eff = torch.as_tensor(self.efficiency[idx], dtype=torch.float32)\n",
    "        return seq, eff\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chop_cls = lambda x: 0 if x<=50 else 1\n",
    "data_train[\"ori_cla\"]=data_train.EFFICIENCY.apply(chop_cls)\n",
    "data_test[\"ori_cla\"]=data_test.EFFICIENCY.apply(chop_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(tr_fea))\n",
    "vali_size = len(tr_fea) - train_size\n",
    "tr = GeneDataset(tr_fea[:train_size],data_train[\"ori_cla\"][:train_size])\n",
    "validation = GeneDataset(tr_fea[train_size:],data_train[\"ori_cla\"][train_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_size = int(0.8 * len(tr_fea))\n",
    "vali_size = len(tr_fea) - train_size\n",
    "tr = GeneDataset(tr_fea[:train_size],data_train['EFFICIENCY'][:train_size]/100)\n",
    "validation = GeneDataset(tr_fea[train_size:],data_train['EFFICIENCY'][train_size:]/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45284"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vali_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = GeneDataset(test_fea,data_test[\"ori_cla\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chop_trainloader = torch.utils.data.DataLoader(tr, batch_size=128,shuffle=True, num_workers=0)\n",
    "chop_validloader = torch.utils.data.DataLoader(validation, batch_size=vali_size,shuffle=True, num_workers=0)\n",
    "chop_testloader = torch.utils.data.DataLoader(test, batch_size=128,shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 13.5784,  -4.2706,  -2.8988,   4.6056,  -2.8107,   1.2597,  -4.9050,\n",
       "          -1.1582,   4.5699,  -1.7905,  -3.7577,  -0.2078,   2.1379,  -4.3548,\n",
       "           4.5876,   8.4380,  -2.3158,  -0.5777,  -0.2357,   2.6147,  -3.9915,\n",
       "           2.1595,   0.6034,  -3.1596,  -6.8227,   1.8248,  -0.0275,   6.9587,\n",
       "         -11.0836,  -8.5389]), tensor(0.))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch线性回归\n",
    "\n",
    "# 导入库\n",
    "import torch      \n",
    "from torch.autograd import Variable     \n",
    "import torch.nn as nn \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class LogisticModel(nn.Module):\n",
    "    def __init__(self,in_dim, n_class):\n",
    "        super(LogisticModel, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(in_dim, 20)\n",
    "        self.l2 = torch.nn.Linear(20, 10)\n",
    "        self.l3 = torch.nn.Linear(10, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.l1(x))\n",
    "        x = self.sigmoid(self.l2(x))\n",
    "        y_pred = self.sigmoid(self.l3(x))\n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "input_dim = tr[0][0].shape[0]\n",
    "output_dim = 1\n",
    "model = LogisticModel(input_dim,output_dim) \n",
    "\n",
    "# Construct loss function and optimizer\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, trainloader, validloader,tolerance=0.0001):\n",
    "    valid_loss_his = []\n",
    "    train_loss_his = []\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        mean_loss=train_loss/(batch_idx+1)\n",
    "        train_loss_his.append(mean_loss)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for valid, tar in validloader:\n",
    "                valid, tar = valid.to('cpu'), tar.to('cpu')\n",
    "                opt = model(valid)\n",
    "                valid_loss = criterion(opt, tar)\n",
    "        valid_loss_his.append(valid_loss)\n",
    "        print('Train: Epoch: %d| train_loss: %f|valid_loss: %f'% (e, mean_loss, valid_loss))\n",
    "        if e >1 and abs(valid_loss_his[-1]-valid_loss_his[-2])<tolerance:\n",
    "            break\n",
    "    return train_loss_his, valid_loss_his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch: 0| train_loss: 0.685633|valid_loss: 0.683790\n",
      "Train: Epoch: 1| train_loss: 0.681240|valid_loss: 0.680759\n",
      "Train: Epoch: 2| train_loss: 0.677359|valid_loss: 0.675586\n",
      "Train: Epoch: 3| train_loss: 0.670068|valid_loss: 0.665955\n",
      "Train: Epoch: 4| train_loss: 0.657764|valid_loss: 0.651589\n",
      "Train: Epoch: 5| train_loss: 0.643053|valid_loss: 0.637955\n",
      "Train: Epoch: 6| train_loss: 0.631368|valid_loss: 0.628421\n",
      "Train: Epoch: 7| train_loss: 0.622385|valid_loss: 0.619661\n",
      "Train: Epoch: 8| train_loss: 0.612608|valid_loss: 0.608229\n",
      "Train: Epoch: 9| train_loss: 0.599801|valid_loss: 0.593735\n",
      "Train: Epoch: 10| train_loss: 0.584678|valid_loss: 0.577511\n",
      "Train: Epoch: 11| train_loss: 0.568223|valid_loss: 0.560709\n",
      "Train: Epoch: 12| train_loss: 0.551862|valid_loss: 0.544278\n",
      "Train: Epoch: 13| train_loss: 0.536084|valid_loss: 0.528994\n",
      "Train: Epoch: 14| train_loss: 0.521462|valid_loss: 0.515674\n",
      "Train: Epoch: 15| train_loss: 0.508039|valid_loss: 0.502685\n",
      "Train: Epoch: 16| train_loss: 0.495730|valid_loss: 0.490858\n",
      "Train: Epoch: 17| train_loss: 0.484729|valid_loss: 0.481009\n",
      "Train: Epoch: 18| train_loss: 0.475590|valid_loss: 0.472724\n",
      "Train: Epoch: 19| train_loss: 0.468131|valid_loss: 0.465748\n",
      "Train: Epoch: 20| train_loss: 0.461622|valid_loss: 0.460614\n",
      "Train: Epoch: 21| train_loss: 0.456039|valid_loss: 0.455136\n",
      "Train: Epoch: 22| train_loss: 0.451115|valid_loss: 0.450500\n",
      "Train: Epoch: 23| train_loss: 0.446769|valid_loss: 0.445974\n",
      "Train: Epoch: 24| train_loss: 0.442166|valid_loss: 0.441453\n",
      "Train: Epoch: 25| train_loss: 0.437288|valid_loss: 0.436542\n",
      "Train: Epoch: 26| train_loss: 0.432497|valid_loss: 0.432264\n",
      "Train: Epoch: 27| train_loss: 0.428255|valid_loss: 0.427347\n",
      "Train: Epoch: 28| train_loss: 0.423992|valid_loss: 0.423420\n",
      "Train: Epoch: 29| train_loss: 0.419900|valid_loss: 0.419064\n",
      "Train: Epoch: 30| train_loss: 0.416108|valid_loss: 0.415493\n",
      "Train: Epoch: 31| train_loss: 0.412706|valid_loss: 0.411533\n",
      "Train: Epoch: 32| train_loss: 0.409533|valid_loss: 0.408645\n",
      "Train: Epoch: 33| train_loss: 0.406550|valid_loss: 0.406060\n",
      "Train: Epoch: 34| train_loss: 0.403921|valid_loss: 0.403454\n",
      "Train: Epoch: 35| train_loss: 0.401691|valid_loss: 0.401186\n",
      "Train: Epoch: 36| train_loss: 0.399594|valid_loss: 0.399620\n",
      "Train: Epoch: 37| train_loss: 0.397556|valid_loss: 0.397059\n",
      "Train: Epoch: 38| train_loss: 0.395737|valid_loss: 0.396090\n",
      "Train: Epoch: 39| train_loss: 0.394044|valid_loss: 0.394137\n",
      "Train: Epoch: 40| train_loss: 0.392392|valid_loss: 0.391946\n",
      "Train: Epoch: 41| train_loss: 0.391086|valid_loss: 0.391412\n",
      "Train: Epoch: 42| train_loss: 0.389443|valid_loss: 0.388566\n",
      "Train: Epoch: 43| train_loss: 0.388372|valid_loss: 0.388594\n"
     ]
    }
   ],
   "source": [
    "# 可继续训练，寻找最优epoch\n",
    "train_loss_his, valid_loss_his = train(epoch=50, model=model, trainloader=chop_trainloader, validloader=chop_validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chop_pre = model(test_fea)\n",
    "data_test['pre']=chop_pre.detach().numpy().reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chop_pre_cla = lambda x: 0 if x<=0.5 else 1\n",
    "data_test[\"pre_cla\"]=data_test.pre.apply(chop_pre_cla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8219093382326962\n",
      "Precision: 0.8266452170286293\n",
      "Recall: 0.8668541080912215\n",
      "F1-Score: 0.8462723210881865\n"
     ]
    }
   ],
   "source": [
    "chop_Y_te = data_test[\"ori_cla\"].values.tolist()\n",
    "chop_preds = data_test[\"pre_cla\"].values.tolist()\n",
    "print('Accuracy:', accuracy_score(chop_Y_te,chop_preds))\n",
    "print('Precision:', precision_score(chop_Y_te,chop_preds))\n",
    "print('Recall:', recall_score(chop_Y_te,chop_preds))\n",
    "print('F1-Score:', f1_score(chop_Y_te,chop_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GUIDE</th>\n",
       "      <th>EFFICIENCY</th>\n",
       "      <th>CHR</th>\n",
       "      <th>STRAND</th>\n",
       "      <th>TSS</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ori_cla</th>\n",
       "      <th>pre</th>\n",
       "      <th>pre_cla</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229866</th>\n",
       "      <td>ATTAGTACGCGAACTCATAGCGG</td>\n",
       "      <td>55.34</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>62340897</td>\n",
       "      <td>62340956</td>\n",
       "      <td>1</td>\n",
       "      <td>0.964052</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110483</th>\n",
       "      <td>ACTGCAGTCGCGATTGGAGGAGG</td>\n",
       "      <td>44.18</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>49498262</td>\n",
       "      <td>49498344</td>\n",
       "      <td>0</td>\n",
       "      <td>0.868224</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13230</th>\n",
       "      <td>TCGGACAAGAAACTAGCTTTGGG</td>\n",
       "      <td>48.11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>15875002</td>\n",
       "      <td>15874883</td>\n",
       "      <td>0</td>\n",
       "      <td>0.176927</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244662</th>\n",
       "      <td>CAGCGGAGAAGAGGAGGCCGTGG</td>\n",
       "      <td>48.86</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>13491501</td>\n",
       "      <td>13491645</td>\n",
       "      <td>0</td>\n",
       "      <td>0.296259</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222274</th>\n",
       "      <td>TAAATATGTAAAATATTTCAAGG</td>\n",
       "      <td>29.31</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>32274383</td>\n",
       "      <td>32274243</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071673</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26004</th>\n",
       "      <td>GAGGAGCCGAGCTAGGCGGAAGG</td>\n",
       "      <td>55.70</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>25845556</td>\n",
       "      <td>25845488</td>\n",
       "      <td>1</td>\n",
       "      <td>0.941890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93379</th>\n",
       "      <td>CAGTTCGCGTTTGGTTAAAGTGG</td>\n",
       "      <td>44.76</td>\n",
       "      <td>19</td>\n",
       "      <td>-1</td>\n",
       "      <td>24068242</td>\n",
       "      <td>24068205</td>\n",
       "      <td>0</td>\n",
       "      <td>0.509479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>TTTACACAGCCTTTTTGGCAGGG</td>\n",
       "      <td>52.34</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>22560132</td>\n",
       "      <td>22560055</td>\n",
       "      <td>1</td>\n",
       "      <td>0.756931</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162947</th>\n",
       "      <td>ATCAAAGCACATGCACTGCTGGG</td>\n",
       "      <td>56.71</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>14801830</td>\n",
       "      <td>14801871</td>\n",
       "      <td>1</td>\n",
       "      <td>0.852932</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192409</th>\n",
       "      <td>GATCCTCCTCTGTGATGGTGTGG</td>\n",
       "      <td>53.40</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>6253289</td>\n",
       "      <td>6253415</td>\n",
       "      <td>1</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40951</th>\n",
       "      <td>ACCCTCCCCCAGCCCGCTGCCGG</td>\n",
       "      <td>43.64</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>35908207</td>\n",
       "      <td>35908183</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121630</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>TGACCTCTAAGCTCTTAATGAGG</td>\n",
       "      <td>61.52</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>10729007</td>\n",
       "      <td>10729048</td>\n",
       "      <td>1</td>\n",
       "      <td>0.903032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97174</th>\n",
       "      <td>GTTGAATAAATCCTATATTTTGG</td>\n",
       "      <td>17.67</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>42336314</td>\n",
       "      <td>42336451</td>\n",
       "      <td>0</td>\n",
       "      <td>0.073621</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202222</th>\n",
       "      <td>ATGCCGCTGGATGTCTCCATTGG</td>\n",
       "      <td>44.60</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>34149333</td>\n",
       "      <td>34149390</td>\n",
       "      <td>0</td>\n",
       "      <td>0.415906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131150</th>\n",
       "      <td>GCTACAGCTCTTCAATTGTCAGG</td>\n",
       "      <td>49.42</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>25565346</td>\n",
       "      <td>25565383</td>\n",
       "      <td>0</td>\n",
       "      <td>0.247980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128685</th>\n",
       "      <td>CCTCTTCACTCCTGCATTGAGGG</td>\n",
       "      <td>34.59</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>18331791</td>\n",
       "      <td>18331652</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045490</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>GCTCCCTCCGCGGTGAGGTTTGG</td>\n",
       "      <td>36.26</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>42521980</td>\n",
       "      <td>42521958</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220418</th>\n",
       "      <td>ATTAACACACACTTCTGAGGTGG</td>\n",
       "      <td>62.82</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>26138318</td>\n",
       "      <td>26138164</td>\n",
       "      <td>1</td>\n",
       "      <td>0.959124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282368</th>\n",
       "      <td>CAGACCCTAAAATCCCAAGAGGG</td>\n",
       "      <td>69.06</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>7673857</td>\n",
       "      <td>7673991</td>\n",
       "      <td>1</td>\n",
       "      <td>0.897173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185668</th>\n",
       "      <td>ACCGGAGCCCGAGCGCCCTCTGG</td>\n",
       "      <td>45.26</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>25019369</td>\n",
       "      <td>25019332</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258061</th>\n",
       "      <td>CAATATCTAGTTCAACCACTAGG</td>\n",
       "      <td>61.99</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>69488810</td>\n",
       "      <td>69488700</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915957</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278569</th>\n",
       "      <td>TCAGGCACACAAGACTAAAGCGG</td>\n",
       "      <td>62.17</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>34944605</td>\n",
       "      <td>34944497</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70882</th>\n",
       "      <td>CTCAGCCTCCGTACTGTAGGCGG</td>\n",
       "      <td>53.91</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>28270040</td>\n",
       "      <td>28269884</td>\n",
       "      <td>1</td>\n",
       "      <td>0.881492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179053</th>\n",
       "      <td>TGTTTACAAGGTATTGAAGCAGG</td>\n",
       "      <td>49.27</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>35132064</td>\n",
       "      <td>35132206</td>\n",
       "      <td>0</td>\n",
       "      <td>0.682714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156095</th>\n",
       "      <td>TACCTGTTCCCGTAGATCGCAGG</td>\n",
       "      <td>52.56</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>32334152</td>\n",
       "      <td>32334055</td>\n",
       "      <td>1</td>\n",
       "      <td>0.478749</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219158</th>\n",
       "      <td>GACTGCCCGGTGGTGCTGCTGGG</td>\n",
       "      <td>30.82</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>23024017</td>\n",
       "      <td>23024146</td>\n",
       "      <td>0</td>\n",
       "      <td>0.229319</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216189</th>\n",
       "      <td>CGCCCTCTTGTGTCGCTCATCGG</td>\n",
       "      <td>37.56</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9592534</td>\n",
       "      <td>9592396</td>\n",
       "      <td>0</td>\n",
       "      <td>0.171870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273347</th>\n",
       "      <td>CCATTTGGGGCTCTTCCGACTGG</td>\n",
       "      <td>41.38</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>18735208</td>\n",
       "      <td>18735341</td>\n",
       "      <td>0</td>\n",
       "      <td>0.697594</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262671</th>\n",
       "      <td>GATTAGACATGCAGGTCCTAAGG</td>\n",
       "      <td>42.43</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>21195396</td>\n",
       "      <td>21195543</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2824</th>\n",
       "      <td>TTGATTTTTATCCAGTTTTCAGG</td>\n",
       "      <td>13.93</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>22772153</td>\n",
       "      <td>22772190</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156411</th>\n",
       "      <td>TGGCCATTCAGTGGGTCGGGTGG</td>\n",
       "      <td>50.75</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>33752307</td>\n",
       "      <td>33752391</td>\n",
       "      <td>1</td>\n",
       "      <td>0.920636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184445</th>\n",
       "      <td>CAGGTCAATGTTCGCCCTCAGGG</td>\n",
       "      <td>49.05</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21847816</td>\n",
       "      <td>21847952</td>\n",
       "      <td>0</td>\n",
       "      <td>0.306152</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>AAAAAAGTACTTCGCCTTGAAGG</td>\n",
       "      <td>31.62</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>20392698</td>\n",
       "      <td>20392580</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105142</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88711</th>\n",
       "      <td>GACCTGGTTTCAGCACTGCAGGG</td>\n",
       "      <td>64.60</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>39473025</td>\n",
       "      <td>39472993</td>\n",
       "      <td>1</td>\n",
       "      <td>0.758185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207876</th>\n",
       "      <td>CGCTCGGATTTCAGCTGGTCCGG</td>\n",
       "      <td>30.07</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>6417280</td>\n",
       "      <td>6417269</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60417</th>\n",
       "      <td>AGTTCACGAAAAGAAAACATTGG</td>\n",
       "      <td>62.98</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>20364626</td>\n",
       "      <td>20364620</td>\n",
       "      <td>1</td>\n",
       "      <td>0.928780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43670</th>\n",
       "      <td>TTTTTCCAACAAGATGTTCCAGG</td>\n",
       "      <td>33.47</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>45967184</td>\n",
       "      <td>45967301</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080834</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160047</th>\n",
       "      <td>TGGTACACGTTAGTGCTTAATGG</td>\n",
       "      <td>33.19</td>\n",
       "      <td>23</td>\n",
       "      <td>-1</td>\n",
       "      <td>44565804</td>\n",
       "      <td>44565711</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248056</th>\n",
       "      <td>AGTGAGAGATCGATCACGATTGG</td>\n",
       "      <td>51.60</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>26132643</td>\n",
       "      <td>26132496</td>\n",
       "      <td>1</td>\n",
       "      <td>0.662301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98201</th>\n",
       "      <td>CCACACTCTTCCCATGCGTATGG</td>\n",
       "      <td>52.64</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>45962057</td>\n",
       "      <td>45962041</td>\n",
       "      <td>1</td>\n",
       "      <td>0.599070</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56586</th>\n",
       "      <td>CTCAGAGAACACCGCGTATGAGG</td>\n",
       "      <td>50.35</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5385838</td>\n",
       "      <td>5385961</td>\n",
       "      <td>1</td>\n",
       "      <td>0.752199</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252302</th>\n",
       "      <td>TCAGCCGTATGGGAGATTAACGG</td>\n",
       "      <td>44.90</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>40205411</td>\n",
       "      <td>40205312</td>\n",
       "      <td>0</td>\n",
       "      <td>0.261413</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96296</th>\n",
       "      <td>ACGGCTGCTGTTGGTCTCTATGG</td>\n",
       "      <td>38.68</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>3874200</td>\n",
       "      <td>3874230</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57598</th>\n",
       "      <td>CAGAGGAAGTTTCACCAGTGCGG</td>\n",
       "      <td>72.61</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>7891769</td>\n",
       "      <td>7891730</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956320</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233522</th>\n",
       "      <td>CCATACTCACATGCTGTCGTGGG</td>\n",
       "      <td>60.09</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>13783635</td>\n",
       "      <td>13783589</td>\n",
       "      <td>1</td>\n",
       "      <td>0.831960</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51530</th>\n",
       "      <td>ATGAGGTCTGAGCATCCCGGTGG</td>\n",
       "      <td>72.79</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>32852455</td>\n",
       "      <td>32852462</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994954</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232494</th>\n",
       "      <td>CACCTGTTGAATGTCAAATGAGG</td>\n",
       "      <td>59.08</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9216754</td>\n",
       "      <td>9216584</td>\n",
       "      <td>1</td>\n",
       "      <td>0.927829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258953</th>\n",
       "      <td>AGCTGTATCCTATCTACAGTCGG</td>\n",
       "      <td>67.31</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>756968</td>\n",
       "      <td>756896</td>\n",
       "      <td>1</td>\n",
       "      <td>0.959031</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85774</th>\n",
       "      <td>CACGTACCTGGTCACGTGCCAGG</td>\n",
       "      <td>46.12</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>19990402</td>\n",
       "      <td>19990358</td>\n",
       "      <td>0</td>\n",
       "      <td>0.159012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234308</th>\n",
       "      <td>ACTTCTGGTACAGAAGGCGGCGG</td>\n",
       "      <td>65.01</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>18128786</td>\n",
       "      <td>18128879</td>\n",
       "      <td>1</td>\n",
       "      <td>0.984874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223368</th>\n",
       "      <td>CGCGTTCAACAGCAGAGAGCAGG</td>\n",
       "      <td>46.26</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3501910</td>\n",
       "      <td>3501896</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700126</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64326</th>\n",
       "      <td>TAACGTGTACAATCCGTCGGTGG</td>\n",
       "      <td>63.83</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>35252538</td>\n",
       "      <td>35252419</td>\n",
       "      <td>1</td>\n",
       "      <td>0.973203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87398</th>\n",
       "      <td>AAGTTAGTAGCTTTACGTGTTGG</td>\n",
       "      <td>49.73</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>27571497</td>\n",
       "      <td>27571646</td>\n",
       "      <td>0</td>\n",
       "      <td>0.583149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50042</th>\n",
       "      <td>CGCAATACGCACCTTATTGGTGG</td>\n",
       "      <td>44.38</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>26704841</td>\n",
       "      <td>26704705</td>\n",
       "      <td>0</td>\n",
       "      <td>0.907118</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58153</th>\n",
       "      <td>TTGTACAGACTTCCTGAAGCGGG</td>\n",
       "      <td>56.77</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>970829</td>\n",
       "      <td>970951</td>\n",
       "      <td>1</td>\n",
       "      <td>0.317788</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232791</th>\n",
       "      <td>ACAGAGTTCCAGCAGAAGTCAGG</td>\n",
       "      <td>46.99</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>10828881</td>\n",
       "      <td>10828852</td>\n",
       "      <td>0</td>\n",
       "      <td>0.441245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196632</th>\n",
       "      <td>TGGTAAATAACAATTCTAGGCGG</td>\n",
       "      <td>68.78</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19665349</td>\n",
       "      <td>19665289</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950747</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154340</th>\n",
       "      <td>TGAAGTAACTCACCAAACACTGG</td>\n",
       "      <td>55.47</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>2789390</td>\n",
       "      <td>2789237</td>\n",
       "      <td>1</td>\n",
       "      <td>0.847747</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98521</th>\n",
       "      <td>CGCAGTATGGAGTATTTGATCGG</td>\n",
       "      <td>42.88</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>47405880</td>\n",
       "      <td>47405976</td>\n",
       "      <td>0</td>\n",
       "      <td>0.330180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97147</th>\n",
       "      <td>CAAAAAGGGGGTTGGGTGCGAGG</td>\n",
       "      <td>47.77</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>42328430</td>\n",
       "      <td>42328392</td>\n",
       "      <td>0</td>\n",
       "      <td>0.905968</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56606 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          GUIDE  EFFICIENCY  CHR  STRAND       TSS  LOCATION  \\\n",
       "229866  ATTAGTACGCGAACTCATAGCGG       55.34    5      -1  62340897  62340956   \n",
       "110483  ACTGCAGTCGCGATTGGAGGAGG       44.18    1      -1  49498262  49498344   \n",
       "13230   TCGGACAAGAAACTAGCTTTGGG       48.11   11       1  15875002  15874883   \n",
       "244662  CAGCGGAGAAGAGGAGGCCGTGG       48.86    7       1  13491501  13491645   \n",
       "222274  TAAATATGTAAAATATTTCAAGG       29.31    5      -1  32274383  32274243   \n",
       "26004   GAGGAGCCGAGCTAGGCGGAAGG       55.70   12      -1  25845556  25845488   \n",
       "93379   CAGTTCGCGTTTGGTTAAAGTGG       44.76   19      -1  24068242  24068205   \n",
       "24997   TTTACACAGCCTTTTTGGCAGGG       52.34   12      -1  22560132  22560055   \n",
       "162947  ATCAAAGCACATGCACTGCTGGG       56.71   24       1  14801830  14801871   \n",
       "192409  GATCCTCCTCTGTGATGGTGTGG       53.40    2      -1   6253289   6253415   \n",
       "40951   ACCCTCCCCCAGCCCGCTGCCGG       43.64   13       1  35908207  35908183   \n",
       "201     TGACCTCTAAGCTCTTAATGAGG       61.52   10      -1  10729007  10729048   \n",
       "97174   GTTGAATAAATCCTATATTTTGG       17.67   19       1  42336314  42336451   \n",
       "202222  ATGCCGCTGGATGTCTCCATTGG       44.60    3      -1  34149333  34149390   \n",
       "131150  GCTACAGCTCTTCAATTGTCAGG       49.42   21      -1  25565346  25565383   \n",
       "128685  CCTCTTCACTCCTGCATTGAGGG       34.59   21       1  18331791  18331652   \n",
       "8585    GCTCCCTCCGCGGTGAGGTTTGG       36.26   10       1  42521980  42521958   \n",
       "220418  ATTAACACACACTTCTGAGGTGG       62.82    5       1  26138318  26138164   \n",
       "282368  CAGACCCTAAAATCCCAAGAGGG       69.06    9      -1   7673857   7673991   \n",
       "185668  ACCGGAGCCCGAGCGCCCTCTGG       45.26    2      -1  25019369  25019332   \n",
       "258061  CAATATCTAGTTCAACCACTAGG       61.99    7       1  69488810  69488700   \n",
       "278569  TCAGGCACACAAGACTAAAGCGG       62.17    9       1  34944605  34944497   \n",
       "70882   CTCAGCCTCCGTACTGTAGGCGG       53.91   16       1  28270040  28269884   \n",
       "179053  TGTTTACAAGGTATTGAAGCAGG       49.27   25       1  35132064  35132206   \n",
       "156095  TACCTGTTCCCGTAGATCGCAGG       52.56   23       1  32334152  32334055   \n",
       "219158  GACTGCCCGGTGGTGCTGCTGGG       30.82    5      -1  23024017  23024146   \n",
       "216189  CGCCCTCTTGTGTCGCTCATCGG       37.56    4       1   9592534   9592396   \n",
       "273347  CCATTTGGGGCTCTTCCGACTGG       41.38    9       1  18735208  18735341   \n",
       "262671  GATTAGACATGCAGGTCCTAAGG       42.43    8      -1  21195396  21195543   \n",
       "2824    TTGATTTTTATCCAGTTTTCAGG       13.93   10      -1  22772153  22772190   \n",
       "...                         ...         ...  ...     ...       ...       ...   \n",
       "156411  TGGCCATTCAGTGGGTCGGGTGG       50.75   23       1  33752307  33752391   \n",
       "184445  CAGGTCAATGTTCGCCCTCAGGG       49.05    2       1  21847816  21847952   \n",
       "2331    AAAAAAGTACTTCGCCTTGAAGG       31.62   10      -1  20392698  20392580   \n",
       "88711   GACCTGGTTTCAGCACTGCAGGG       64.60   18      -1  39473025  39472993   \n",
       "207876  CGCTCGGATTTCAGCTGGTCCGG       30.07    3      -1   6417280   6417269   \n",
       "60417   AGTTCACGAAAAGAAAACATTGG       62.98   15       1  20364626  20364620   \n",
       "43670   TTTTTCCAACAAGATGTTCCAGG       33.47   13       1  45967184  45967301   \n",
       "160047  TGGTACACGTTAGTGCTTAATGG       33.19   23      -1  44565804  44565711   \n",
       "248056  AGTGAGAGATCGATCACGATTGG       51.60    7       1  26132643  26132496   \n",
       "98201   CCACACTCTTCCCATGCGTATGG       52.64   19       1  45962057  45962041   \n",
       "56586   CTCAGAGAACACCGCGTATGAGG       50.35   14       1   5385838   5385961   \n",
       "252302  TCAGCCGTATGGGAGATTAACGG       44.90    7      -1  40205411  40205312   \n",
       "96296   ACGGCTGCTGTTGGTCTCTATGG       38.68   19       1   3874200   3874230   \n",
       "57598   CAGAGGAAGTTTCACCAGTGCGG       72.61   14      -1   7891769   7891730   \n",
       "233522  CCATACTCACATGCTGTCGTGGG       60.09    6       1  13783635  13783589   \n",
       "51530   ATGAGGTCTGAGCATCCCGGTGG       72.79   14      -1  32852455  32852462   \n",
       "232494  CACCTGTTGAATGTCAAATGAGG       59.08    5       1   9216754   9216584   \n",
       "258953  AGCTGTATCCTATCTACAGTCGG       67.31    7       1    756968    756896   \n",
       "85774   CACGTACCTGGTCACGTGCCAGG       46.12   18       1  19990402  19990358   \n",
       "234308  ACTTCTGGTACAGAAGGCGGCGG       65.01    6      -1  18128786  18128879   \n",
       "223368  CGCGTTCAACAGCAGAGAGCAGG       46.26    5       1   3501910   3501896   \n",
       "64326   TAACGTGTACAATCCGTCGGTGG       63.83   15      -1  35252538  35252419   \n",
       "87398   AAGTTAGTAGCTTTACGTGTTGG       49.73   18      -1  27571497  27571646   \n",
       "50042   CGCAATACGCACCTTATTGGTGG       44.38   14       1  26704841  26704705   \n",
       "58153   TTGTACAGACTTCCTGAAGCGGG       56.77   14       1    970829    970951   \n",
       "232791  ACAGAGTTCCAGCAGAAGTCAGG       46.99    6      -1  10828881  10828852   \n",
       "196632  TGGTAAATAACAATTCTAGGCGG       68.78    3       1  19665349  19665289   \n",
       "154340  TGAAGTAACTCACCAAACACTGG       55.47   23       1   2789390   2789237   \n",
       "98521   CGCAGTATGGAGTATTTGATCGG       42.88   19       1  47405880  47405976   \n",
       "97147   CAAAAAGGGGGTTGGGTGCGAGG       47.77   19       1  42328430  42328392   \n",
       "\n",
       "        ori_cla       pre  pre_cla  \n",
       "229866        1  0.964052        1  \n",
       "110483        0  0.868224        1  \n",
       "13230         0  0.176927        0  \n",
       "244662        0  0.296259        0  \n",
       "222274        0  0.071673        0  \n",
       "26004         1  0.941890        1  \n",
       "93379         0  0.509479        1  \n",
       "24997         1  0.756931        1  \n",
       "162947        1  0.852932        1  \n",
       "192409        1  0.398058        0  \n",
       "40951         0  0.121630        0  \n",
       "201           1  0.903032        1  \n",
       "97174         0  0.073621        0  \n",
       "202222        0  0.415906        0  \n",
       "131150        0  0.247980        0  \n",
       "128685        0  0.045490        0  \n",
       "8585          0  0.202292        0  \n",
       "220418        1  0.959124        1  \n",
       "282368        1  0.897173        1  \n",
       "185668        0  0.150446        0  \n",
       "258061        1  0.915957        1  \n",
       "278569        1  0.835938        1  \n",
       "70882         1  0.881492        1  \n",
       "179053        0  0.682714        1  \n",
       "156095        1  0.478749        0  \n",
       "219158        0  0.229319        0  \n",
       "216189        0  0.171870        0  \n",
       "273347        0  0.697594        1  \n",
       "262671        0  0.138028        0  \n",
       "2824          0  0.005926        0  \n",
       "...         ...       ...      ...  \n",
       "156411        1  0.920636        1  \n",
       "184445        0  0.306152        0  \n",
       "2331          0  0.105142        0  \n",
       "88711         1  0.758185        1  \n",
       "207876        0  0.033818        0  \n",
       "60417         1  0.928780        1  \n",
       "43670         0  0.080834        0  \n",
       "160047        0  0.089002        0  \n",
       "248056        1  0.662301        1  \n",
       "98201         1  0.599070        1  \n",
       "56586         1  0.752199        1  \n",
       "252302        0  0.261413        0  \n",
       "96296         0  0.138580        0  \n",
       "57598         1  0.956320        1  \n",
       "233522        1  0.831960        1  \n",
       "51530         1  0.994954        1  \n",
       "232494        1  0.927829        1  \n",
       "258953        1  0.959031        1  \n",
       "85774         0  0.159012        0  \n",
       "234308        1  0.984874        1  \n",
       "223368        0  0.700126        1  \n",
       "64326         1  0.973203        1  \n",
       "87398         0  0.583149        1  \n",
       "50042         0  0.907118        1  \n",
       "58153         1  0.317788        0  \n",
       "232791        0  0.441245        0  \n",
       "196632        1  0.950747        1  \n",
       "154340        1  0.847747        1  \n",
       "98521         0  0.330180        0  \n",
       "97147         0  0.905968        1  \n",
       "\n",
       "[56606 rows x 9 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on local data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data = pd.read_csv(\"danrer11_guide_results.txt\", sep=\"\\t\",usecols=[\"GUIDE\",\"qPCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = lambda x: 0 if x<=0 else (1 if x>=1 else (x))\n",
    "loc_data[\"qPCR\"]=loc_data.qPCR.apply(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deal with the not unregular guide sequence that the length are not 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(loc_data['GUIDE'])):\n",
    "    if len(loc_data['GUIDE'][i])!=23:\n",
    "        if len(loc_data['GUIDE'][i])==20:\n",
    "            loc_data['GUIDE'][i]=loc_data['GUIDE'][i]+'TGG'     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data = loc_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dataset = GGEDataset(data = batch_one_hot(loc_data,encoding), transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(loc_dataset, batch_size=len(loc_dataset),shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([63, 30])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        loc_fea = net.encoder(data.view(-1,1*4*23).to(device)).to('cpu')\n",
    "print(\"Data shape: {}\".format(loc_fea.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDataset(object):\n",
    "    def __init__(self, guide, eff):\n",
    "        self.target_sequence = guide\n",
    "        self.efficiency = eff.values\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.target_sequence[idx]\n",
    "        eff = torch.as_tensor(self.efficiency[idx], dtype=torch.float32)\n",
    "        return seq, eff\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(loc_dataset))\n",
    "test_size = len(loc_data) - train_size\n",
    "cls = lambda x: 0 if x<=0.5 else 1\n",
    "loc_data[\"ori_cla\"]=loc_data.qPCR.apply(cls)\n",
    "\n",
    "loc_tr = GeneDataset(loc_fea[:train_size],loc_data[\"ori_cla\"][:train_size])\n",
    "loc_test = GeneDataset(loc_fea[train_size:],loc_data[\"ori_cla\"][train_size:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_data = loc_data[train_size:]\n",
    "test_fea = loc_fea[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_trainloader = torch.utils.data.DataLoader(loc_tr, batch_size=train_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "loc_testloader = torch.utils.data.DataLoader(loc_test, batch_size=test_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4.3966,  1.4377, -1.9444, -3.8962,  0.0272,  1.0193,  0.7438,  2.0559,\n",
       "         -9.6917,  5.3428,  3.0753, -5.5811, -0.6464, -0.0710, -0.7752, -0.5107,\n",
       "         -2.7819,  6.3033, 10.3760,  9.8890,  3.8949,  9.2581,  5.6374, -2.2282,\n",
       "          3.2854,  5.9708, -0.5337,  8.7561, -5.6170, 12.7319]), tensor(1.))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_tr[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### directly using the model trained from chopchop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data['Y_pre_no_tl'] = model(loc_fea).detach().numpy().reshape(1,-1)[0].tolist()\n",
    "loc_data[\"Y_pre_no_tl_cla\"]=loc_data.Y_pre_no_tl.apply(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.31746031746031744\n",
      "Precision: 0.2777777777777778\n",
      "Recall: 0.7894736842105263\n",
      "F1-Score: 0.410958904109589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# fill in your code...\n",
    "Y_te = loc_data[\"ori_cla\"].values.tolist()\n",
    "preds_te_no_tl = loc_data[\"Y_pre_no_tl_cla\"].values.tolist()\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_no_tl))\n",
    "print('Precision:', precision_score(Y_te,preds_te_no_tl))\n",
    "print('Recall:', recall_score(Y_te,preds_te_no_tl))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_no_tl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without using transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model on loc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch线性回归\n",
    "# 导入库\n",
    "import torch      \n",
    "from torch.autograd import Variable     \n",
    "import torch.nn as nn \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class LogisticModel(nn.Module):\n",
    "    def __init__(self,in_dim, n_class):\n",
    "        super(LogisticModel, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(in_dim, 20)\n",
    "        self.l2 = torch.nn.Linear(20, 10)\n",
    "        self.l3 = torch.nn.Linear(10, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.l1(x))\n",
    "        x = self.sigmoid(self.l2(x))\n",
    "        y_pred = self.sigmoid(self.l3(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "input_dim = 30\n",
    "output_dim = 1\n",
    "Lmodel = LogisticModel(input_dim,output_dim) \n",
    "\n",
    "# Construct loss function and optimizer\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(Lmodel.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_train(epoch, model, trainloader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print('Train: Epoch: %d| train_loss: %f'% (epoch, train_loss))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, testloader):\n",
    "    # global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "        print('-----------------------------------------------Test: Epoch: %d| test loss: %.3f'% (epoch, test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch: 0| train_loss: 0.649180\n",
      "-----------------------------------------------Test: Epoch: 0| test loss: 0.685\n",
      "Train: Epoch: 1| train_loss: 0.592922\n",
      "-----------------------------------------------Test: Epoch: 1| test loss: 0.700\n",
      "Train: Epoch: 2| train_loss: 0.573409\n",
      "-----------------------------------------------Test: Epoch: 2| test loss: 0.714\n",
      "Train: Epoch: 3| train_loss: 0.565941\n",
      "-----------------------------------------------Test: Epoch: 3| test loss: 0.724\n",
      "Train: Epoch: 4| train_loss: 0.562807\n",
      "-----------------------------------------------Test: Epoch: 4| test loss: 0.731\n",
      "Train: Epoch: 5| train_loss: 0.561337\n",
      "-----------------------------------------------Test: Epoch: 5| test loss: 0.736\n",
      "Train: Epoch: 6| train_loss: 0.560536\n",
      "-----------------------------------------------Test: Epoch: 6| test loss: 0.739\n",
      "Train: Epoch: 7| train_loss: 0.560008\n",
      "-----------------------------------------------Test: Epoch: 7| test loss: 0.741\n",
      "Train: Epoch: 8| train_loss: 0.559594\n",
      "-----------------------------------------------Test: Epoch: 8| test loss: 0.742\n",
      "Train: Epoch: 9| train_loss: 0.559224\n",
      "-----------------------------------------------Test: Epoch: 9| test loss: 0.743\n",
      "Train: Epoch: 10| train_loss: 0.558870\n",
      "-----------------------------------------------Test: Epoch: 10| test loss: 0.744\n",
      "Train: Epoch: 11| train_loss: 0.558518\n",
      "-----------------------------------------------Test: Epoch: 11| test loss: 0.744\n",
      "Train: Epoch: 12| train_loss: 0.558162\n",
      "-----------------------------------------------Test: Epoch: 12| test loss: 0.744\n",
      "Train: Epoch: 13| train_loss: 0.557799\n",
      "-----------------------------------------------Test: Epoch: 13| test loss: 0.744\n",
      "Train: Epoch: 14| train_loss: 0.557426\n",
      "-----------------------------------------------Test: Epoch: 14| test loss: 0.744\n",
      "Train: Epoch: 15| train_loss: 0.557043\n",
      "-----------------------------------------------Test: Epoch: 15| test loss: 0.744\n",
      "Train: Epoch: 16| train_loss: 0.556649\n",
      "-----------------------------------------------Test: Epoch: 16| test loss: 0.744\n",
      "Train: Epoch: 17| train_loss: 0.556243\n",
      "-----------------------------------------------Test: Epoch: 17| test loss: 0.744\n",
      "Train: Epoch: 18| train_loss: 0.555824\n",
      "-----------------------------------------------Test: Epoch: 18| test loss: 0.744\n",
      "Train: Epoch: 19| train_loss: 0.555394\n",
      "-----------------------------------------------Test: Epoch: 19| test loss: 0.744\n",
      "Train: Epoch: 20| train_loss: 0.554951\n",
      "-----------------------------------------------Test: Epoch: 20| test loss: 0.743\n",
      "Train: Epoch: 21| train_loss: 0.554496\n",
      "-----------------------------------------------Test: Epoch: 21| test loss: 0.743\n",
      "Train: Epoch: 22| train_loss: 0.554029\n",
      "-----------------------------------------------Test: Epoch: 22| test loss: 0.743\n",
      "Train: Epoch: 23| train_loss: 0.553550\n",
      "-----------------------------------------------Test: Epoch: 23| test loss: 0.743\n",
      "Train: Epoch: 24| train_loss: 0.553059\n",
      "-----------------------------------------------Test: Epoch: 24| test loss: 0.743\n",
      "Train: Epoch: 25| train_loss: 0.552555\n",
      "-----------------------------------------------Test: Epoch: 25| test loss: 0.742\n",
      "Train: Epoch: 26| train_loss: 0.552037\n",
      "-----------------------------------------------Test: Epoch: 26| test loss: 0.742\n",
      "Train: Epoch: 27| train_loss: 0.551504\n",
      "-----------------------------------------------Test: Epoch: 27| test loss: 0.742\n",
      "Train: Epoch: 28| train_loss: 0.550954\n",
      "-----------------------------------------------Test: Epoch: 28| test loss: 0.741\n",
      "Train: Epoch: 29| train_loss: 0.550387\n",
      "-----------------------------------------------Test: Epoch: 29| test loss: 0.741\n",
      "Train: Epoch: 30| train_loss: 0.549800\n",
      "-----------------------------------------------Test: Epoch: 30| test loss: 0.741\n",
      "Train: Epoch: 31| train_loss: 0.549192\n",
      "-----------------------------------------------Test: Epoch: 31| test loss: 0.740\n",
      "Train: Epoch: 32| train_loss: 0.548562\n",
      "-----------------------------------------------Test: Epoch: 32| test loss: 0.740\n",
      "Train: Epoch: 33| train_loss: 0.547908\n",
      "-----------------------------------------------Test: Epoch: 33| test loss: 0.740\n",
      "Train: Epoch: 34| train_loss: 0.547228\n",
      "-----------------------------------------------Test: Epoch: 34| test loss: 0.739\n",
      "Train: Epoch: 35| train_loss: 0.546521\n",
      "-----------------------------------------------Test: Epoch: 35| test loss: 0.739\n",
      "Train: Epoch: 36| train_loss: 0.545783\n",
      "-----------------------------------------------Test: Epoch: 36| test loss: 0.738\n",
      "Train: Epoch: 37| train_loss: 0.545015\n",
      "-----------------------------------------------Test: Epoch: 37| test loss: 0.738\n",
      "Train: Epoch: 38| train_loss: 0.544213\n",
      "-----------------------------------------------Test: Epoch: 38| test loss: 0.737\n",
      "Train: Epoch: 39| train_loss: 0.543376\n",
      "-----------------------------------------------Test: Epoch: 39| test loss: 0.737\n",
      "Train: Epoch: 40| train_loss: 0.542502\n",
      "-----------------------------------------------Test: Epoch: 40| test loss: 0.736\n",
      "Train: Epoch: 41| train_loss: 0.541587\n",
      "-----------------------------------------------Test: Epoch: 41| test loss: 0.736\n",
      "Train: Epoch: 42| train_loss: 0.540631\n",
      "-----------------------------------------------Test: Epoch: 42| test loss: 0.735\n",
      "Train: Epoch: 43| train_loss: 0.539631\n",
      "-----------------------------------------------Test: Epoch: 43| test loss: 0.735\n",
      "Train: Epoch: 44| train_loss: 0.538583\n",
      "-----------------------------------------------Test: Epoch: 44| test loss: 0.734\n",
      "Train: Epoch: 45| train_loss: 0.537486\n",
      "-----------------------------------------------Test: Epoch: 45| test loss: 0.733\n",
      "Train: Epoch: 46| train_loss: 0.536337\n",
      "-----------------------------------------------Test: Epoch: 46| test loss: 0.732\n",
      "Train: Epoch: 47| train_loss: 0.535133\n",
      "-----------------------------------------------Test: Epoch: 47| test loss: 0.732\n",
      "Train: Epoch: 48| train_loss: 0.533870\n",
      "-----------------------------------------------Test: Epoch: 48| test loss: 0.731\n",
      "Train: Epoch: 49| train_loss: 0.532547\n",
      "-----------------------------------------------Test: Epoch: 49| test loss: 0.730\n",
      "Train: Epoch: 50| train_loss: 0.531159\n",
      "-----------------------------------------------Test: Epoch: 50| test loss: 0.729\n",
      "Train: Epoch: 51| train_loss: 0.529705\n",
      "-----------------------------------------------Test: Epoch: 51| test loss: 0.728\n",
      "Train: Epoch: 52| train_loss: 0.528179\n",
      "-----------------------------------------------Test: Epoch: 52| test loss: 0.727\n",
      "Train: Epoch: 53| train_loss: 0.526579\n",
      "-----------------------------------------------Test: Epoch: 53| test loss: 0.726\n",
      "Train: Epoch: 54| train_loss: 0.524900\n",
      "-----------------------------------------------Test: Epoch: 54| test loss: 0.725\n",
      "Train: Epoch: 55| train_loss: 0.523140\n",
      "-----------------------------------------------Test: Epoch: 55| test loss: 0.724\n",
      "Train: Epoch: 56| train_loss: 0.521294\n",
      "-----------------------------------------------Test: Epoch: 56| test loss: 0.723\n",
      "Train: Epoch: 57| train_loss: 0.519357\n",
      "-----------------------------------------------Test: Epoch: 57| test loss: 0.721\n",
      "Train: Epoch: 58| train_loss: 0.517326\n",
      "-----------------------------------------------Test: Epoch: 58| test loss: 0.720\n",
      "Train: Epoch: 59| train_loss: 0.515195\n",
      "-----------------------------------------------Test: Epoch: 59| test loss: 0.719\n",
      "Train: Epoch: 60| train_loss: 0.512959\n",
      "-----------------------------------------------Test: Epoch: 60| test loss: 0.717\n",
      "Train: Epoch: 61| train_loss: 0.510615\n",
      "-----------------------------------------------Test: Epoch: 61| test loss: 0.716\n",
      "Train: Epoch: 62| train_loss: 0.508156\n",
      "-----------------------------------------------Test: Epoch: 62| test loss: 0.714\n",
      "Train: Epoch: 63| train_loss: 0.505580\n",
      "-----------------------------------------------Test: Epoch: 63| test loss: 0.712\n",
      "Train: Epoch: 64| train_loss: 0.502882\n",
      "-----------------------------------------------Test: Epoch: 64| test loss: 0.711\n",
      "Train: Epoch: 65| train_loss: 0.500059\n",
      "-----------------------------------------------Test: Epoch: 65| test loss: 0.709\n",
      "Train: Epoch: 66| train_loss: 0.497107\n",
      "-----------------------------------------------Test: Epoch: 66| test loss: 0.707\n",
      "Train: Epoch: 67| train_loss: 0.494027\n",
      "-----------------------------------------------Test: Epoch: 67| test loss: 0.705\n",
      "Train: Epoch: 68| train_loss: 0.490817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------Test: Epoch: 68| test loss: 0.703\n",
      "Train: Epoch: 69| train_loss: 0.487478\n",
      "-----------------------------------------------Test: Epoch: 69| test loss: 0.700\n",
      "Train: Epoch: 70| train_loss: 0.484012\n",
      "-----------------------------------------------Test: Epoch: 70| test loss: 0.698\n",
      "Train: Epoch: 71| train_loss: 0.480420\n",
      "-----------------------------------------------Test: Epoch: 71| test loss: 0.696\n",
      "Train: Epoch: 72| train_loss: 0.476704\n",
      "-----------------------------------------------Test: Epoch: 72| test loss: 0.693\n",
      "Train: Epoch: 73| train_loss: 0.472867\n",
      "-----------------------------------------------Test: Epoch: 73| test loss: 0.691\n",
      "Train: Epoch: 74| train_loss: 0.468910\n",
      "-----------------------------------------------Test: Epoch: 74| test loss: 0.688\n",
      "Train: Epoch: 75| train_loss: 0.464836\n",
      "-----------------------------------------------Test: Epoch: 75| test loss: 0.686\n",
      "Train: Epoch: 76| train_loss: 0.460645\n",
      "-----------------------------------------------Test: Epoch: 76| test loss: 0.683\n",
      "Train: Epoch: 77| train_loss: 0.456339\n",
      "-----------------------------------------------Test: Epoch: 77| test loss: 0.680\n",
      "Train: Epoch: 78| train_loss: 0.451919\n",
      "-----------------------------------------------Test: Epoch: 78| test loss: 0.678\n",
      "Train: Epoch: 79| train_loss: 0.447387\n",
      "-----------------------------------------------Test: Epoch: 79| test loss: 0.675\n",
      "Train: Epoch: 80| train_loss: 0.442744\n",
      "-----------------------------------------------Test: Epoch: 80| test loss: 0.672\n",
      "Train: Epoch: 81| train_loss: 0.437993\n",
      "-----------------------------------------------Test: Epoch: 81| test loss: 0.670\n",
      "Train: Epoch: 82| train_loss: 0.433136\n",
      "-----------------------------------------------Test: Epoch: 82| test loss: 0.667\n",
      "Train: Epoch: 83| train_loss: 0.428177\n",
      "-----------------------------------------------Test: Epoch: 83| test loss: 0.665\n",
      "Train: Epoch: 84| train_loss: 0.423118\n",
      "-----------------------------------------------Test: Epoch: 84| test loss: 0.662\n",
      "Train: Epoch: 85| train_loss: 0.417962\n",
      "-----------------------------------------------Test: Epoch: 85| test loss: 0.660\n",
      "Train: Epoch: 86| train_loss: 0.412712\n",
      "-----------------------------------------------Test: Epoch: 86| test loss: 0.658\n",
      "Train: Epoch: 87| train_loss: 0.407371\n",
      "-----------------------------------------------Test: Epoch: 87| test loss: 0.656\n",
      "Train: Epoch: 88| train_loss: 0.401944\n",
      "-----------------------------------------------Test: Epoch: 88| test loss: 0.654\n",
      "Train: Epoch: 89| train_loss: 0.396433\n",
      "-----------------------------------------------Test: Epoch: 89| test loss: 0.652\n",
      "Train: Epoch: 90| train_loss: 0.390843\n",
      "-----------------------------------------------Test: Epoch: 90| test loss: 0.650\n",
      "Train: Epoch: 91| train_loss: 0.385179\n",
      "-----------------------------------------------Test: Epoch: 91| test loss: 0.648\n",
      "Train: Epoch: 92| train_loss: 0.379450\n",
      "-----------------------------------------------Test: Epoch: 92| test loss: 0.647\n",
      "Train: Epoch: 93| train_loss: 0.373664\n",
      "-----------------------------------------------Test: Epoch: 93| test loss: 0.646\n",
      "Train: Epoch: 94| train_loss: 0.367830\n",
      "-----------------------------------------------Test: Epoch: 94| test loss: 0.644\n",
      "Train: Epoch: 95| train_loss: 0.361958\n",
      "-----------------------------------------------Test: Epoch: 95| test loss: 0.643\n",
      "Train: Epoch: 96| train_loss: 0.356055\n",
      "-----------------------------------------------Test: Epoch: 96| test loss: 0.643\n",
      "Train: Epoch: 97| train_loss: 0.350127\n",
      "-----------------------------------------------Test: Epoch: 97| test loss: 0.642\n",
      "Train: Epoch: 98| train_loss: 0.344180\n",
      "-----------------------------------------------Test: Epoch: 98| test loss: 0.641\n",
      "Train: Epoch: 99| train_loss: 0.338215\n",
      "-----------------------------------------------Test: Epoch: 99| test loss: 0.641\n",
      "Train: Epoch: 100| train_loss: 0.332233\n",
      "-----------------------------------------------Test: Epoch: 100| test loss: 0.641\n",
      "Train: Epoch: 101| train_loss: 0.326234\n",
      "-----------------------------------------------Test: Epoch: 101| test loss: 0.641\n",
      "Train: Epoch: 102| train_loss: 0.320217\n",
      "-----------------------------------------------Test: Epoch: 102| test loss: 0.641\n",
      "Train: Epoch: 103| train_loss: 0.314182\n",
      "-----------------------------------------------Test: Epoch: 103| test loss: 0.641\n",
      "Train: Epoch: 104| train_loss: 0.308133\n",
      "-----------------------------------------------Test: Epoch: 104| test loss: 0.642\n",
      "Train: Epoch: 105| train_loss: 0.302072\n",
      "-----------------------------------------------Test: Epoch: 105| test loss: 0.643\n",
      "Train: Epoch: 106| train_loss: 0.296007\n",
      "-----------------------------------------------Test: Epoch: 106| test loss: 0.643\n",
      "Train: Epoch: 107| train_loss: 0.289945\n",
      "-----------------------------------------------Test: Epoch: 107| test loss: 0.644\n",
      "Train: Epoch: 108| train_loss: 0.283893\n",
      "-----------------------------------------------Test: Epoch: 108| test loss: 0.645\n",
      "Train: Epoch: 109| train_loss: 0.277859\n",
      "-----------------------------------------------Test: Epoch: 109| test loss: 0.646\n",
      "Train: Epoch: 110| train_loss: 0.271844\n",
      "-----------------------------------------------Test: Epoch: 110| test loss: 0.647\n",
      "Train: Epoch: 111| train_loss: 0.265846\n",
      "-----------------------------------------------Test: Epoch: 111| test loss: 0.648\n",
      "Train: Epoch: 112| train_loss: 0.259861\n",
      "-----------------------------------------------Test: Epoch: 112| test loss: 0.650\n",
      "Train: Epoch: 113| train_loss: 0.253894\n",
      "-----------------------------------------------Test: Epoch: 113| test loss: 0.651\n",
      "Train: Epoch: 114| train_loss: 0.247967\n",
      "-----------------------------------------------Test: Epoch: 114| test loss: 0.653\n",
      "Train: Epoch: 115| train_loss: 0.242120\n",
      "-----------------------------------------------Test: Epoch: 115| test loss: 0.655\n",
      "Train: Epoch: 116| train_loss: 0.236386\n",
      "-----------------------------------------------Test: Epoch: 116| test loss: 0.657\n",
      "Train: Epoch: 117| train_loss: 0.230774\n",
      "-----------------------------------------------Test: Epoch: 117| test loss: 0.659\n",
      "Train: Epoch: 118| train_loss: 0.225277\n",
      "-----------------------------------------------Test: Epoch: 118| test loss: 0.661\n",
      "Train: Epoch: 119| train_loss: 0.219886\n",
      "-----------------------------------------------Test: Epoch: 119| test loss: 0.664\n",
      "Train: Epoch: 120| train_loss: 0.214591\n",
      "-----------------------------------------------Test: Epoch: 120| test loss: 0.667\n",
      "Train: Epoch: 121| train_loss: 0.209384\n",
      "-----------------------------------------------Test: Epoch: 121| test loss: 0.670\n",
      "Train: Epoch: 122| train_loss: 0.204258\n",
      "-----------------------------------------------Test: Epoch: 122| test loss: 0.673\n",
      "Train: Epoch: 123| train_loss: 0.199209\n",
      "-----------------------------------------------Test: Epoch: 123| test loss: 0.677\n",
      "Train: Epoch: 124| train_loss: 0.194233\n",
      "-----------------------------------------------Test: Epoch: 124| test loss: 0.682\n",
      "Train: Epoch: 125| train_loss: 0.189326\n",
      "-----------------------------------------------Test: Epoch: 125| test loss: 0.687\n",
      "Train: Epoch: 126| train_loss: 0.184469\n",
      "-----------------------------------------------Test: Epoch: 126| test loss: 0.692\n",
      "Train: Epoch: 127| train_loss: 0.179631\n",
      "-----------------------------------------------Test: Epoch: 127| test loss: 0.699\n",
      "Train: Epoch: 128| train_loss: 0.174756\n",
      "-----------------------------------------------Test: Epoch: 128| test loss: 0.706\n",
      "Train: Epoch: 129| train_loss: 0.169804\n",
      "-----------------------------------------------Test: Epoch: 129| test loss: 0.714\n",
      "Train: Epoch: 130| train_loss: 0.164861\n",
      "-----------------------------------------------Test: Epoch: 130| test loss: 0.720\n",
      "Train: Epoch: 131| train_loss: 0.160124\n",
      "-----------------------------------------------Test: Epoch: 131| test loss: 0.726\n",
      "Train: Epoch: 132| train_loss: 0.155657\n",
      "-----------------------------------------------Test: Epoch: 132| test loss: 0.731\n",
      "Train: Epoch: 133| train_loss: 0.151398\n",
      "-----------------------------------------------Test: Epoch: 133| test loss: 0.735\n",
      "Train: Epoch: 134| train_loss: 0.147300\n",
      "-----------------------------------------------Test: Epoch: 134| test loss: 0.740\n",
      "Train: Epoch: 135| train_loss: 0.143339\n",
      "-----------------------------------------------Test: Epoch: 135| test loss: 0.744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch: 136| train_loss: 0.139502\n",
      "-----------------------------------------------Test: Epoch: 136| test loss: 0.749\n",
      "Train: Epoch: 137| train_loss: 0.135781\n",
      "-----------------------------------------------Test: Epoch: 137| test loss: 0.754\n",
      "Train: Epoch: 138| train_loss: 0.132170\n",
      "-----------------------------------------------Test: Epoch: 138| test loss: 0.759\n",
      "Train: Epoch: 139| train_loss: 0.128664\n",
      "-----------------------------------------------Test: Epoch: 139| test loss: 0.764\n",
      "Train: Epoch: 140| train_loss: 0.125259\n",
      "-----------------------------------------------Test: Epoch: 140| test loss: 0.769\n",
      "Train: Epoch: 141| train_loss: 0.121952\n",
      "-----------------------------------------------Test: Epoch: 141| test loss: 0.775\n",
      "Train: Epoch: 142| train_loss: 0.118737\n",
      "-----------------------------------------------Test: Epoch: 142| test loss: 0.780\n",
      "Train: Epoch: 143| train_loss: 0.115610\n",
      "-----------------------------------------------Test: Epoch: 143| test loss: 0.786\n",
      "Train: Epoch: 144| train_loss: 0.112567\n",
      "-----------------------------------------------Test: Epoch: 144| test loss: 0.792\n",
      "Train: Epoch: 145| train_loss: 0.109603\n",
      "-----------------------------------------------Test: Epoch: 145| test loss: 0.798\n",
      "Train: Epoch: 146| train_loss: 0.106714\n",
      "-----------------------------------------------Test: Epoch: 146| test loss: 0.804\n",
      "Train: Epoch: 147| train_loss: 0.103899\n",
      "-----------------------------------------------Test: Epoch: 147| test loss: 0.810\n",
      "Train: Epoch: 148| train_loss: 0.101156\n",
      "-----------------------------------------------Test: Epoch: 148| test loss: 0.817\n",
      "Train: Epoch: 149| train_loss: 0.098487\n",
      "-----------------------------------------------Test: Epoch: 149| test loss: 0.823\n",
      "Train: Epoch: 150| train_loss: 0.095895\n",
      "-----------------------------------------------Test: Epoch: 150| test loss: 0.830\n",
      "Train: Epoch: 151| train_loss: 0.093385\n",
      "-----------------------------------------------Test: Epoch: 151| test loss: 0.837\n",
      "Train: Epoch: 152| train_loss: 0.090959\n",
      "-----------------------------------------------Test: Epoch: 152| test loss: 0.844\n",
      "Train: Epoch: 153| train_loss: 0.088618\n",
      "-----------------------------------------------Test: Epoch: 153| test loss: 0.850\n",
      "Train: Epoch: 154| train_loss: 0.086362\n",
      "-----------------------------------------------Test: Epoch: 154| test loss: 0.857\n",
      "Train: Epoch: 155| train_loss: 0.084189\n",
      "-----------------------------------------------Test: Epoch: 155| test loss: 0.864\n",
      "Train: Epoch: 156| train_loss: 0.082097\n",
      "-----------------------------------------------Test: Epoch: 156| test loss: 0.871\n",
      "Train: Epoch: 157| train_loss: 0.080081\n",
      "-----------------------------------------------Test: Epoch: 157| test loss: 0.878\n",
      "Train: Epoch: 158| train_loss: 0.078139\n",
      "-----------------------------------------------Test: Epoch: 158| test loss: 0.885\n",
      "Train: Epoch: 159| train_loss: 0.076266\n",
      "-----------------------------------------------Test: Epoch: 159| test loss: 0.891\n",
      "Train: Epoch: 160| train_loss: 0.074460\n",
      "-----------------------------------------------Test: Epoch: 160| test loss: 0.898\n",
      "Train: Epoch: 161| train_loss: 0.072717\n",
      "-----------------------------------------------Test: Epoch: 161| test loss: 0.904\n",
      "Train: Epoch: 162| train_loss: 0.071033\n",
      "-----------------------------------------------Test: Epoch: 162| test loss: 0.911\n",
      "Train: Epoch: 163| train_loss: 0.069406\n",
      "-----------------------------------------------Test: Epoch: 163| test loss: 0.917\n",
      "Train: Epoch: 164| train_loss: 0.067833\n",
      "-----------------------------------------------Test: Epoch: 164| test loss: 0.924\n",
      "Train: Epoch: 165| train_loss: 0.066310\n",
      "-----------------------------------------------Test: Epoch: 165| test loss: 0.930\n",
      "Train: Epoch: 166| train_loss: 0.064834\n",
      "-----------------------------------------------Test: Epoch: 166| test loss: 0.936\n",
      "Train: Epoch: 167| train_loss: 0.063401\n",
      "-----------------------------------------------Test: Epoch: 167| test loss: 0.943\n",
      "Train: Epoch: 168| train_loss: 0.062009\n",
      "-----------------------------------------------Test: Epoch: 168| test loss: 0.949\n",
      "Train: Epoch: 169| train_loss: 0.060652\n",
      "-----------------------------------------------Test: Epoch: 169| test loss: 0.956\n",
      "Train: Epoch: 170| train_loss: 0.059327\n",
      "-----------------------------------------------Test: Epoch: 170| test loss: 0.962\n",
      "Train: Epoch: 171| train_loss: 0.058028\n",
      "-----------------------------------------------Test: Epoch: 171| test loss: 0.968\n",
      "Train: Epoch: 172| train_loss: 0.056748\n",
      "-----------------------------------------------Test: Epoch: 172| test loss: 0.975\n",
      "Train: Epoch: 173| train_loss: 0.055477\n",
      "-----------------------------------------------Test: Epoch: 173| test loss: 0.982\n",
      "Train: Epoch: 174| train_loss: 0.054201\n",
      "-----------------------------------------------Test: Epoch: 174| test loss: 0.989\n",
      "Train: Epoch: 175| train_loss: 0.052902\n",
      "-----------------------------------------------Test: Epoch: 175| test loss: 0.996\n",
      "Train: Epoch: 176| train_loss: 0.051557\n",
      "-----------------------------------------------Test: Epoch: 176| test loss: 1.003\n",
      "Train: Epoch: 177| train_loss: 0.050157\n",
      "-----------------------------------------------Test: Epoch: 177| test loss: 1.010\n",
      "Train: Epoch: 178| train_loss: 0.048743\n",
      "-----------------------------------------------Test: Epoch: 178| test loss: 1.016\n",
      "Train: Epoch: 179| train_loss: 0.047405\n",
      "-----------------------------------------------Test: Epoch: 179| test loss: 1.021\n",
      "Train: Epoch: 180| train_loss: 0.046202\n",
      "-----------------------------------------------Test: Epoch: 180| test loss: 1.024\n",
      "Train: Epoch: 181| train_loss: 0.045117\n",
      "-----------------------------------------------Test: Epoch: 181| test loss: 1.028\n",
      "Train: Epoch: 182| train_loss: 0.044117\n",
      "-----------------------------------------------Test: Epoch: 182| test loss: 1.031\n",
      "Train: Epoch: 183| train_loss: 0.043180\n",
      "-----------------------------------------------Test: Epoch: 183| test loss: 1.035\n",
      "Train: Epoch: 184| train_loss: 0.042292\n",
      "-----------------------------------------------Test: Epoch: 184| test loss: 1.038\n",
      "Train: Epoch: 185| train_loss: 0.041447\n",
      "-----------------------------------------------Test: Epoch: 185| test loss: 1.042\n",
      "Train: Epoch: 186| train_loss: 0.040640\n",
      "-----------------------------------------------Test: Epoch: 186| test loss: 1.045\n",
      "Train: Epoch: 187| train_loss: 0.039867\n",
      "-----------------------------------------------Test: Epoch: 187| test loss: 1.049\n",
      "Train: Epoch: 188| train_loss: 0.039125\n",
      "-----------------------------------------------Test: Epoch: 188| test loss: 1.052\n",
      "Train: Epoch: 189| train_loss: 0.038412\n",
      "-----------------------------------------------Test: Epoch: 189| test loss: 1.056\n",
      "Train: Epoch: 190| train_loss: 0.037725\n",
      "-----------------------------------------------Test: Epoch: 190| test loss: 1.059\n",
      "Train: Epoch: 191| train_loss: 0.037063\n",
      "-----------------------------------------------Test: Epoch: 191| test loss: 1.063\n",
      "Train: Epoch: 192| train_loss: 0.036425\n",
      "-----------------------------------------------Test: Epoch: 192| test loss: 1.067\n",
      "Train: Epoch: 193| train_loss: 0.035808\n",
      "-----------------------------------------------Test: Epoch: 193| test loss: 1.070\n",
      "Train: Epoch: 194| train_loss: 0.035211\n",
      "-----------------------------------------------Test: Epoch: 194| test loss: 1.074\n",
      "Train: Epoch: 195| train_loss: 0.034633\n",
      "-----------------------------------------------Test: Epoch: 195| test loss: 1.078\n",
      "Train: Epoch: 196| train_loss: 0.034074\n",
      "-----------------------------------------------Test: Epoch: 196| test loss: 1.081\n",
      "Train: Epoch: 197| train_loss: 0.033531\n",
      "-----------------------------------------------Test: Epoch: 197| test loss: 1.085\n",
      "Train: Epoch: 198| train_loss: 0.033005\n",
      "-----------------------------------------------Test: Epoch: 198| test loss: 1.089\n",
      "Train: Epoch: 199| train_loss: 0.032494\n",
      "-----------------------------------------------Test: Epoch: 199| test loss: 1.092\n"
     ]
    }
   ],
   "source": [
    "tr_loss = []\n",
    "te_loss = []\n",
    "for e in range(200):\n",
    "    loss = loc_train(epoch=e, model=Lmodel, trainloader=loc_trainloader)\n",
    "    tr_loss.append(loss)\n",
    "    teloss = test(epoch=e, model=Lmodel, testloader=loc_testloader)\n",
    "    te_loss.append(teloss)\n",
    "    #if e >1 and abs(te_loss[-1]-te_loss[-2])<0.0001:\n",
    "    #    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = list(range(1,len(tr_loss)+1))\n",
    "plt.plot(x,tr_loss, label='training loss')\n",
    "plt.plot(x,te_loss, label='test loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('loc Training Monitoring')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on all loc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Lmodel(loc_fea)\n",
    "loc_data['pre']=pre.detach().numpy().reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = lambda x: 0 if x<=0.5 else 1\n",
    "# loc_data[\"ori_cla\"]=loc_data.qPCR.apply(cls)\n",
    "loc_data[\"pre_cla\"]=loc_data.pre.apply(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8571428571428571\n",
      "Precision: 0.8571428571428571\n",
      "Recall: 0.631578947368421\n",
      "F1-Score: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "Y_te = loc_data[\"ori_cla\"].values.tolist()\n",
    "preds_te = loc_data[\"pre_cla\"].values.tolist()\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te))\n",
    "print('Recall:', recall_score(Y_te,preds_te))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on loc test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5263157894736842\n",
      "Precision: 0.3333333333333333\n",
      "Recall: 0.125\n",
      "F1-Score: 0.18181818181818182\n"
     ]
    }
   ],
   "source": [
    "testpre = Lmodel(test_fea)\n",
    "test_data['pre'] = testpre.detach().numpy().reshape(1,-1)[0].tolist()\n",
    "\n",
    "cls = lambda x: 0 if x<=0.5 else 1\n",
    "test_data[\"pre_cla\"]=test_data.pre.apply(cls)\n",
    "# fill in your code...\n",
    "Y_te = test_data[\"ori_cla\"].values.tolist()\n",
    "preds_te = test_data[\"pre_cla\"].values.tolist()\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te))\n",
    "print('Recall:', recall_score(Y_te,preds_te))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticModel(\n",
      "  (l1): Linear(in_features=30, out_features=20, bias=True)\n",
      "  (l2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (l3): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model.l3.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.l3 =  torch.nn.Sequential(nn.Linear(num_ftrs, 5),\n",
    "                                    nn.Sigmoid(),\n",
    "                                       torch.nn.Linear(5, 1))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticModel(\n",
       "  (l1): Linear(in_features=30, out_features=20, bias=True)\n",
       "  (l2): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (l3): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE/均方差\n",
    "#criterion = nn.MSELoss()\n",
    "\n",
    "# 优化(找到最小化差值的参数)\n",
    "#learning_rate = 0.1  # 学习率, 达到最佳参数的速度有多快\n",
    "#optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_train(epoch, model, trainloader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        print('Train: Epoch: %d| train_loss: %f'% (epoch, train_loss))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch: 0| train_loss: 0.621484\n",
      "-----------------------------------------------Test: Epoch: 0| test loss: 0.679\n",
      "Train: Epoch: 1| train_loss: 0.620876\n",
      "-----------------------------------------------Test: Epoch: 1| test loss: 0.679\n",
      "Train: Epoch: 2| train_loss: 0.620273\n",
      "-----------------------------------------------Test: Epoch: 2| test loss: 0.679\n",
      "Train: Epoch: 3| train_loss: 0.619678\n",
      "-----------------------------------------------Test: Epoch: 3| test loss: 0.679\n",
      "Train: Epoch: 4| train_loss: 0.619089\n",
      "-----------------------------------------------Test: Epoch: 4| test loss: 0.679\n",
      "Train: Epoch: 5| train_loss: 0.618507\n",
      "-----------------------------------------------Test: Epoch: 5| test loss: 0.679\n",
      "Train: Epoch: 6| train_loss: 0.617931\n",
      "-----------------------------------------------Test: Epoch: 6| test loss: 0.679\n",
      "Train: Epoch: 7| train_loss: 0.617361\n",
      "-----------------------------------------------Test: Epoch: 7| test loss: 0.679\n",
      "Train: Epoch: 8| train_loss: 0.616797\n",
      "-----------------------------------------------Test: Epoch: 8| test loss: 0.680\n",
      "Train: Epoch: 9| train_loss: 0.616240\n",
      "-----------------------------------------------Test: Epoch: 9| test loss: 0.680\n",
      "Train: Epoch: 10| train_loss: 0.615689\n",
      "-----------------------------------------------Test: Epoch: 10| test loss: 0.680\n",
      "Train: Epoch: 11| train_loss: 0.615143\n",
      "-----------------------------------------------Test: Epoch: 11| test loss: 0.680\n",
      "Train: Epoch: 12| train_loss: 0.614604\n",
      "-----------------------------------------------Test: Epoch: 12| test loss: 0.680\n",
      "Train: Epoch: 13| train_loss: 0.614071\n",
      "-----------------------------------------------Test: Epoch: 13| test loss: 0.680\n",
      "Train: Epoch: 14| train_loss: 0.613543\n",
      "-----------------------------------------------Test: Epoch: 14| test loss: 0.680\n",
      "Train: Epoch: 15| train_loss: 0.613021\n",
      "-----------------------------------------------Test: Epoch: 15| test loss: 0.680\n",
      "Train: Epoch: 16| train_loss: 0.612505\n",
      "-----------------------------------------------Test: Epoch: 16| test loss: 0.680\n",
      "Train: Epoch: 17| train_loss: 0.611995\n",
      "-----------------------------------------------Test: Epoch: 17| test loss: 0.680\n",
      "Train: Epoch: 18| train_loss: 0.611489\n",
      "-----------------------------------------------Test: Epoch: 18| test loss: 0.680\n",
      "Train: Epoch: 19| train_loss: 0.610990\n",
      "-----------------------------------------------Test: Epoch: 19| test loss: 0.680\n",
      "Train: Epoch: 20| train_loss: 0.610496\n",
      "-----------------------------------------------Test: Epoch: 20| test loss: 0.680\n",
      "Train: Epoch: 21| train_loss: 0.610007\n",
      "-----------------------------------------------Test: Epoch: 21| test loss: 0.680\n",
      "Train: Epoch: 22| train_loss: 0.609523\n",
      "-----------------------------------------------Test: Epoch: 22| test loss: 0.681\n",
      "Train: Epoch: 23| train_loss: 0.609045\n",
      "-----------------------------------------------Test: Epoch: 23| test loss: 0.681\n",
      "Train: Epoch: 24| train_loss: 0.608572\n",
      "-----------------------------------------------Test: Epoch: 24| test loss: 0.681\n",
      "Train: Epoch: 25| train_loss: 0.608104\n",
      "-----------------------------------------------Test: Epoch: 25| test loss: 0.681\n",
      "Train: Epoch: 26| train_loss: 0.607641\n",
      "-----------------------------------------------Test: Epoch: 26| test loss: 0.681\n",
      "Train: Epoch: 27| train_loss: 0.607183\n",
      "-----------------------------------------------Test: Epoch: 27| test loss: 0.681\n",
      "Train: Epoch: 28| train_loss: 0.606730\n",
      "-----------------------------------------------Test: Epoch: 28| test loss: 0.681\n",
      "Train: Epoch: 29| train_loss: 0.606282\n",
      "-----------------------------------------------Test: Epoch: 29| test loss: 0.681\n",
      "Train: Epoch: 30| train_loss: 0.605838\n",
      "-----------------------------------------------Test: Epoch: 30| test loss: 0.681\n",
      "Train: Epoch: 31| train_loss: 0.605400\n",
      "-----------------------------------------------Test: Epoch: 31| test loss: 0.681\n",
      "Train: Epoch: 32| train_loss: 0.604966\n",
      "-----------------------------------------------Test: Epoch: 32| test loss: 0.681\n",
      "Train: Epoch: 33| train_loss: 0.604536\n",
      "-----------------------------------------------Test: Epoch: 33| test loss: 0.682\n",
      "Train: Epoch: 34| train_loss: 0.604112\n",
      "-----------------------------------------------Test: Epoch: 34| test loss: 0.682\n",
      "Train: Epoch: 35| train_loss: 0.603692\n",
      "-----------------------------------------------Test: Epoch: 35| test loss: 0.682\n",
      "Train: Epoch: 36| train_loss: 0.603276\n",
      "-----------------------------------------------Test: Epoch: 36| test loss: 0.682\n",
      "Train: Epoch: 37| train_loss: 0.602865\n",
      "-----------------------------------------------Test: Epoch: 37| test loss: 0.682\n",
      "Train: Epoch: 38| train_loss: 0.602458\n",
      "-----------------------------------------------Test: Epoch: 38| test loss: 0.682\n",
      "Train: Epoch: 39| train_loss: 0.602056\n",
      "-----------------------------------------------Test: Epoch: 39| test loss: 0.682\n",
      "Train: Epoch: 40| train_loss: 0.601658\n",
      "-----------------------------------------------Test: Epoch: 40| test loss: 0.682\n",
      "Train: Epoch: 41| train_loss: 0.601264\n",
      "-----------------------------------------------Test: Epoch: 41| test loss: 0.683\n",
      "Train: Epoch: 42| train_loss: 0.600874\n",
      "-----------------------------------------------Test: Epoch: 42| test loss: 0.683\n",
      "Train: Epoch: 43| train_loss: 0.600489\n",
      "-----------------------------------------------Test: Epoch: 43| test loss: 0.683\n",
      "Train: Epoch: 44| train_loss: 0.600107\n",
      "-----------------------------------------------Test: Epoch: 44| test loss: 0.683\n",
      "Train: Epoch: 45| train_loss: 0.599730\n",
      "-----------------------------------------------Test: Epoch: 45| test loss: 0.683\n",
      "Train: Epoch: 46| train_loss: 0.599356\n",
      "-----------------------------------------------Test: Epoch: 46| test loss: 0.683\n",
      "Train: Epoch: 47| train_loss: 0.598987\n",
      "-----------------------------------------------Test: Epoch: 47| test loss: 0.683\n",
      "Train: Epoch: 48| train_loss: 0.598621\n",
      "-----------------------------------------------Test: Epoch: 48| test loss: 0.683\n",
      "Train: Epoch: 49| train_loss: 0.598260\n",
      "-----------------------------------------------Test: Epoch: 49| test loss: 0.684\n",
      "Train: Epoch: 50| train_loss: 0.597902\n",
      "-----------------------------------------------Test: Epoch: 50| test loss: 0.684\n",
      "Train: Epoch: 51| train_loss: 0.597548\n",
      "-----------------------------------------------Test: Epoch: 51| test loss: 0.684\n",
      "Train: Epoch: 52| train_loss: 0.597198\n",
      "-----------------------------------------------Test: Epoch: 52| test loss: 0.684\n",
      "Train: Epoch: 53| train_loss: 0.596851\n",
      "-----------------------------------------------Test: Epoch: 53| test loss: 0.684\n",
      "Train: Epoch: 54| train_loss: 0.596508\n",
      "-----------------------------------------------Test: Epoch: 54| test loss: 0.684\n",
      "Train: Epoch: 55| train_loss: 0.596169\n",
      "-----------------------------------------------Test: Epoch: 55| test loss: 0.684\n",
      "Train: Epoch: 56| train_loss: 0.595833\n",
      "-----------------------------------------------Test: Epoch: 56| test loss: 0.684\n",
      "Train: Epoch: 57| train_loss: 0.595501\n",
      "-----------------------------------------------Test: Epoch: 57| test loss: 0.685\n",
      "Train: Epoch: 58| train_loss: 0.595172\n",
      "-----------------------------------------------Test: Epoch: 58| test loss: 0.685\n",
      "Train: Epoch: 59| train_loss: 0.594847\n",
      "-----------------------------------------------Test: Epoch: 59| test loss: 0.685\n",
      "Train: Epoch: 60| train_loss: 0.594525\n",
      "-----------------------------------------------Test: Epoch: 60| test loss: 0.685\n",
      "Train: Epoch: 61| train_loss: 0.594207\n",
      "-----------------------------------------------Test: Epoch: 61| test loss: 0.685\n",
      "Train: Epoch: 62| train_loss: 0.593892\n",
      "-----------------------------------------------Test: Epoch: 62| test loss: 0.685\n",
      "Train: Epoch: 63| train_loss: 0.593580\n",
      "-----------------------------------------------Test: Epoch: 63| test loss: 0.685\n",
      "Train: Epoch: 64| train_loss: 0.593272\n",
      "-----------------------------------------------Test: Epoch: 64| test loss: 0.686\n",
      "Train: Epoch: 65| train_loss: 0.592966\n",
      "-----------------------------------------------Test: Epoch: 65| test loss: 0.686\n",
      "Train: Epoch: 66| train_loss: 0.592664\n",
      "-----------------------------------------------Test: Epoch: 66| test loss: 0.686\n",
      "Train: Epoch: 67| train_loss: 0.592365\n",
      "-----------------------------------------------Test: Epoch: 67| test loss: 0.686\n",
      "Train: Epoch: 68| train_loss: 0.592070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------Test: Epoch: 68| test loss: 0.686\n",
      "Train: Epoch: 69| train_loss: 0.591777\n",
      "-----------------------------------------------Test: Epoch: 69| test loss: 0.686\n",
      "Train: Epoch: 70| train_loss: 0.591487\n",
      "-----------------------------------------------Test: Epoch: 70| test loss: 0.687\n",
      "Train: Epoch: 71| train_loss: 0.591201\n",
      "-----------------------------------------------Test: Epoch: 71| test loss: 0.687\n",
      "Train: Epoch: 72| train_loss: 0.590917\n",
      "-----------------------------------------------Test: Epoch: 72| test loss: 0.687\n",
      "Train: Epoch: 73| train_loss: 0.590636\n",
      "-----------------------------------------------Test: Epoch: 73| test loss: 0.687\n",
      "Train: Epoch: 74| train_loss: 0.590358\n",
      "-----------------------------------------------Test: Epoch: 74| test loss: 0.687\n",
      "Train: Epoch: 75| train_loss: 0.590084\n",
      "-----------------------------------------------Test: Epoch: 75| test loss: 0.687\n",
      "Train: Epoch: 76| train_loss: 0.589812\n",
      "-----------------------------------------------Test: Epoch: 76| test loss: 0.687\n",
      "Train: Epoch: 77| train_loss: 0.589543\n",
      "-----------------------------------------------Test: Epoch: 77| test loss: 0.688\n",
      "Train: Epoch: 78| train_loss: 0.589276\n",
      "-----------------------------------------------Test: Epoch: 78| test loss: 0.688\n",
      "Train: Epoch: 79| train_loss: 0.589012\n",
      "-----------------------------------------------Test: Epoch: 79| test loss: 0.688\n",
      "Train: Epoch: 80| train_loss: 0.588751\n",
      "-----------------------------------------------Test: Epoch: 80| test loss: 0.688\n",
      "Train: Epoch: 81| train_loss: 0.588493\n",
      "-----------------------------------------------Test: Epoch: 81| test loss: 0.688\n",
      "Train: Epoch: 82| train_loss: 0.588238\n",
      "-----------------------------------------------Test: Epoch: 82| test loss: 0.688\n",
      "Train: Epoch: 83| train_loss: 0.587985\n",
      "-----------------------------------------------Test: Epoch: 83| test loss: 0.689\n",
      "Train: Epoch: 84| train_loss: 0.587735\n",
      "-----------------------------------------------Test: Epoch: 84| test loss: 0.689\n",
      "Train: Epoch: 85| train_loss: 0.587487\n",
      "-----------------------------------------------Test: Epoch: 85| test loss: 0.689\n",
      "Train: Epoch: 86| train_loss: 0.587242\n",
      "-----------------------------------------------Test: Epoch: 86| test loss: 0.689\n",
      "Train: Epoch: 87| train_loss: 0.586999\n",
      "-----------------------------------------------Test: Epoch: 87| test loss: 0.689\n",
      "Train: Epoch: 88| train_loss: 0.586759\n",
      "-----------------------------------------------Test: Epoch: 88| test loss: 0.689\n",
      "Train: Epoch: 89| train_loss: 0.586521\n",
      "-----------------------------------------------Test: Epoch: 89| test loss: 0.689\n",
      "Train: Epoch: 90| train_loss: 0.586286\n",
      "-----------------------------------------------Test: Epoch: 90| test loss: 0.690\n",
      "Train: Epoch: 91| train_loss: 0.586053\n",
      "-----------------------------------------------Test: Epoch: 91| test loss: 0.690\n",
      "Train: Epoch: 92| train_loss: 0.585823\n",
      "-----------------------------------------------Test: Epoch: 92| test loss: 0.690\n",
      "Train: Epoch: 93| train_loss: 0.585595\n",
      "-----------------------------------------------Test: Epoch: 93| test loss: 0.690\n",
      "Train: Epoch: 94| train_loss: 0.585370\n",
      "-----------------------------------------------Test: Epoch: 94| test loss: 0.690\n",
      "Train: Epoch: 95| train_loss: 0.585146\n",
      "-----------------------------------------------Test: Epoch: 95| test loss: 0.690\n",
      "Train: Epoch: 96| train_loss: 0.584925\n",
      "-----------------------------------------------Test: Epoch: 96| test loss: 0.691\n",
      "Train: Epoch: 97| train_loss: 0.584706\n",
      "-----------------------------------------------Test: Epoch: 97| test loss: 0.691\n",
      "Train: Epoch: 98| train_loss: 0.584490\n",
      "-----------------------------------------------Test: Epoch: 98| test loss: 0.691\n",
      "Train: Epoch: 99| train_loss: 0.584275\n",
      "-----------------------------------------------Test: Epoch: 99| test loss: 0.691\n",
      "Train: Epoch: 100| train_loss: 0.584063\n",
      "-----------------------------------------------Test: Epoch: 100| test loss: 0.691\n",
      "Train: Epoch: 101| train_loss: 0.583853\n",
      "-----------------------------------------------Test: Epoch: 101| test loss: 0.691\n",
      "Train: Epoch: 102| train_loss: 0.583645\n",
      "-----------------------------------------------Test: Epoch: 102| test loss: 0.692\n",
      "Train: Epoch: 103| train_loss: 0.583440\n",
      "-----------------------------------------------Test: Epoch: 103| test loss: 0.692\n",
      "Train: Epoch: 104| train_loss: 0.583236\n",
      "-----------------------------------------------Test: Epoch: 104| test loss: 0.692\n",
      "Train: Epoch: 105| train_loss: 0.583034\n",
      "-----------------------------------------------Test: Epoch: 105| test loss: 0.692\n",
      "Train: Epoch: 106| train_loss: 0.582835\n",
      "-----------------------------------------------Test: Epoch: 106| test loss: 0.692\n",
      "Train: Epoch: 107| train_loss: 0.582638\n",
      "-----------------------------------------------Test: Epoch: 107| test loss: 0.692\n",
      "Train: Epoch: 108| train_loss: 0.582442\n",
      "-----------------------------------------------Test: Epoch: 108| test loss: 0.693\n",
      "Train: Epoch: 109| train_loss: 0.582249\n",
      "-----------------------------------------------Test: Epoch: 109| test loss: 0.693\n",
      "Train: Epoch: 110| train_loss: 0.582057\n",
      "-----------------------------------------------Test: Epoch: 110| test loss: 0.693\n",
      "Train: Epoch: 111| train_loss: 0.581868\n",
      "-----------------------------------------------Test: Epoch: 111| test loss: 0.693\n",
      "Train: Epoch: 112| train_loss: 0.581680\n",
      "-----------------------------------------------Test: Epoch: 112| test loss: 0.693\n",
      "Train: Epoch: 113| train_loss: 0.581494\n",
      "-----------------------------------------------Test: Epoch: 113| test loss: 0.693\n",
      "Train: Epoch: 114| train_loss: 0.581310\n",
      "-----------------------------------------------Test: Epoch: 114| test loss: 0.694\n",
      "Train: Epoch: 115| train_loss: 0.581128\n",
      "-----------------------------------------------Test: Epoch: 115| test loss: 0.694\n",
      "Train: Epoch: 116| train_loss: 0.580948\n",
      "-----------------------------------------------Test: Epoch: 116| test loss: 0.694\n",
      "Train: Epoch: 117| train_loss: 0.580770\n",
      "-----------------------------------------------Test: Epoch: 117| test loss: 0.694\n",
      "Train: Epoch: 118| train_loss: 0.580593\n",
      "-----------------------------------------------Test: Epoch: 118| test loss: 0.694\n",
      "Train: Epoch: 119| train_loss: 0.580419\n",
      "-----------------------------------------------Test: Epoch: 119| test loss: 0.694\n",
      "Train: Epoch: 120| train_loss: 0.580246\n",
      "-----------------------------------------------Test: Epoch: 120| test loss: 0.695\n",
      "Train: Epoch: 121| train_loss: 0.580074\n",
      "-----------------------------------------------Test: Epoch: 121| test loss: 0.695\n",
      "Train: Epoch: 122| train_loss: 0.579905\n",
      "-----------------------------------------------Test: Epoch: 122| test loss: 0.695\n",
      "Train: Epoch: 123| train_loss: 0.579737\n",
      "-----------------------------------------------Test: Epoch: 123| test loss: 0.695\n",
      "Train: Epoch: 124| train_loss: 0.579571\n",
      "-----------------------------------------------Test: Epoch: 124| test loss: 0.695\n",
      "Train: Epoch: 125| train_loss: 0.579407\n",
      "-----------------------------------------------Test: Epoch: 125| test loss: 0.695\n",
      "Train: Epoch: 126| train_loss: 0.579244\n",
      "-----------------------------------------------Test: Epoch: 126| test loss: 0.696\n",
      "Train: Epoch: 127| train_loss: 0.579083\n",
      "-----------------------------------------------Test: Epoch: 127| test loss: 0.696\n",
      "Train: Epoch: 128| train_loss: 0.578923\n",
      "-----------------------------------------------Test: Epoch: 128| test loss: 0.696\n",
      "Train: Epoch: 129| train_loss: 0.578765\n",
      "-----------------------------------------------Test: Epoch: 129| test loss: 0.696\n",
      "Train: Epoch: 130| train_loss: 0.578609\n",
      "-----------------------------------------------Test: Epoch: 130| test loss: 0.696\n",
      "Train: Epoch: 131| train_loss: 0.578454\n",
      "-----------------------------------------------Test: Epoch: 131| test loss: 0.696\n",
      "Train: Epoch: 132| train_loss: 0.578301\n",
      "-----------------------------------------------Test: Epoch: 132| test loss: 0.697\n",
      "Train: Epoch: 133| train_loss: 0.578149\n",
      "-----------------------------------------------Test: Epoch: 133| test loss: 0.697\n",
      "Train: Epoch: 134| train_loss: 0.577999\n",
      "-----------------------------------------------Test: Epoch: 134| test loss: 0.697\n",
      "Train: Epoch: 135| train_loss: 0.577851\n",
      "-----------------------------------------------Test: Epoch: 135| test loss: 0.697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch: 136| train_loss: 0.577703\n",
      "-----------------------------------------------Test: Epoch: 136| test loss: 0.697\n",
      "Train: Epoch: 137| train_loss: 0.577558\n",
      "-----------------------------------------------Test: Epoch: 137| test loss: 0.697\n",
      "Train: Epoch: 138| train_loss: 0.577413\n",
      "-----------------------------------------------Test: Epoch: 138| test loss: 0.698\n",
      "Train: Epoch: 139| train_loss: 0.577270\n",
      "-----------------------------------------------Test: Epoch: 139| test loss: 0.698\n",
      "Train: Epoch: 140| train_loss: 0.577129\n",
      "-----------------------------------------------Test: Epoch: 140| test loss: 0.698\n",
      "Train: Epoch: 141| train_loss: 0.576989\n",
      "-----------------------------------------------Test: Epoch: 141| test loss: 0.698\n",
      "Train: Epoch: 142| train_loss: 0.576851\n",
      "-----------------------------------------------Test: Epoch: 142| test loss: 0.698\n",
      "Train: Epoch: 143| train_loss: 0.576713\n",
      "-----------------------------------------------Test: Epoch: 143| test loss: 0.698\n",
      "Train: Epoch: 144| train_loss: 0.576578\n",
      "-----------------------------------------------Test: Epoch: 144| test loss: 0.699\n",
      "Train: Epoch: 145| train_loss: 0.576443\n",
      "-----------------------------------------------Test: Epoch: 145| test loss: 0.699\n",
      "Train: Epoch: 146| train_loss: 0.576310\n",
      "-----------------------------------------------Test: Epoch: 146| test loss: 0.699\n",
      "Train: Epoch: 147| train_loss: 0.576178\n",
      "-----------------------------------------------Test: Epoch: 147| test loss: 0.699\n",
      "Train: Epoch: 148| train_loss: 0.576048\n",
      "-----------------------------------------------Test: Epoch: 148| test loss: 0.699\n",
      "Train: Epoch: 149| train_loss: 0.575918\n",
      "-----------------------------------------------Test: Epoch: 149| test loss: 0.699\n",
      "Train: Epoch: 150| train_loss: 0.575791\n",
      "-----------------------------------------------Test: Epoch: 150| test loss: 0.700\n",
      "Train: Epoch: 151| train_loss: 0.575664\n",
      "-----------------------------------------------Test: Epoch: 151| test loss: 0.700\n",
      "Train: Epoch: 152| train_loss: 0.575538\n",
      "-----------------------------------------------Test: Epoch: 152| test loss: 0.700\n",
      "Train: Epoch: 153| train_loss: 0.575414\n",
      "-----------------------------------------------Test: Epoch: 153| test loss: 0.700\n",
      "Train: Epoch: 154| train_loss: 0.575291\n",
      "-----------------------------------------------Test: Epoch: 154| test loss: 0.700\n",
      "Train: Epoch: 155| train_loss: 0.575170\n",
      "-----------------------------------------------Test: Epoch: 155| test loss: 0.700\n",
      "Train: Epoch: 156| train_loss: 0.575049\n",
      "-----------------------------------------------Test: Epoch: 156| test loss: 0.701\n",
      "Train: Epoch: 157| train_loss: 0.574930\n",
      "-----------------------------------------------Test: Epoch: 157| test loss: 0.701\n",
      "Train: Epoch: 158| train_loss: 0.574812\n",
      "-----------------------------------------------Test: Epoch: 158| test loss: 0.701\n",
      "Train: Epoch: 159| train_loss: 0.574695\n",
      "-----------------------------------------------Test: Epoch: 159| test loss: 0.701\n",
      "Train: Epoch: 160| train_loss: 0.574579\n",
      "-----------------------------------------------Test: Epoch: 160| test loss: 0.701\n",
      "Train: Epoch: 161| train_loss: 0.574464\n",
      "-----------------------------------------------Test: Epoch: 161| test loss: 0.701\n",
      "Train: Epoch: 162| train_loss: 0.574351\n",
      "-----------------------------------------------Test: Epoch: 162| test loss: 0.702\n",
      "Train: Epoch: 163| train_loss: 0.574238\n",
      "-----------------------------------------------Test: Epoch: 163| test loss: 0.702\n",
      "Train: Epoch: 164| train_loss: 0.574127\n",
      "-----------------------------------------------Test: Epoch: 164| test loss: 0.702\n",
      "Train: Epoch: 165| train_loss: 0.574016\n",
      "-----------------------------------------------Test: Epoch: 165| test loss: 0.702\n",
      "Train: Epoch: 166| train_loss: 0.573907\n",
      "-----------------------------------------------Test: Epoch: 166| test loss: 0.702\n",
      "Train: Epoch: 167| train_loss: 0.573799\n",
      "-----------------------------------------------Test: Epoch: 167| test loss: 0.702\n",
      "Train: Epoch: 168| train_loss: 0.573692\n",
      "-----------------------------------------------Test: Epoch: 168| test loss: 0.703\n",
      "Train: Epoch: 169| train_loss: 0.573586\n",
      "-----------------------------------------------Test: Epoch: 169| test loss: 0.703\n",
      "Train: Epoch: 170| train_loss: 0.573481\n",
      "-----------------------------------------------Test: Epoch: 170| test loss: 0.703\n",
      "Train: Epoch: 171| train_loss: 0.573378\n",
      "-----------------------------------------------Test: Epoch: 171| test loss: 0.703\n",
      "Train: Epoch: 172| train_loss: 0.573275\n",
      "-----------------------------------------------Test: Epoch: 172| test loss: 0.703\n",
      "Train: Epoch: 173| train_loss: 0.573173\n",
      "-----------------------------------------------Test: Epoch: 173| test loss: 0.703\n",
      "Train: Epoch: 174| train_loss: 0.573072\n",
      "-----------------------------------------------Test: Epoch: 174| test loss: 0.703\n",
      "Train: Epoch: 175| train_loss: 0.572972\n",
      "-----------------------------------------------Test: Epoch: 175| test loss: 0.704\n",
      "Train: Epoch: 176| train_loss: 0.572873\n",
      "-----------------------------------------------Test: Epoch: 176| test loss: 0.704\n",
      "Train: Epoch: 177| train_loss: 0.572775\n",
      "-----------------------------------------------Test: Epoch: 177| test loss: 0.704\n",
      "Train: Epoch: 178| train_loss: 0.572678\n",
      "-----------------------------------------------Test: Epoch: 178| test loss: 0.704\n",
      "Train: Epoch: 179| train_loss: 0.572582\n",
      "-----------------------------------------------Test: Epoch: 179| test loss: 0.704\n",
      "Train: Epoch: 180| train_loss: 0.572487\n",
      "-----------------------------------------------Test: Epoch: 180| test loss: 0.704\n",
      "Train: Epoch: 181| train_loss: 0.572393\n",
      "-----------------------------------------------Test: Epoch: 181| test loss: 0.705\n",
      "Train: Epoch: 182| train_loss: 0.572300\n",
      "-----------------------------------------------Test: Epoch: 182| test loss: 0.705\n",
      "Train: Epoch: 183| train_loss: 0.572207\n",
      "-----------------------------------------------Test: Epoch: 183| test loss: 0.705\n",
      "Train: Epoch: 184| train_loss: 0.572116\n",
      "-----------------------------------------------Test: Epoch: 184| test loss: 0.705\n",
      "Train: Epoch: 185| train_loss: 0.572025\n",
      "-----------------------------------------------Test: Epoch: 185| test loss: 0.705\n",
      "Train: Epoch: 186| train_loss: 0.571936\n",
      "-----------------------------------------------Test: Epoch: 186| test loss: 0.705\n",
      "Train: Epoch: 187| train_loss: 0.571847\n",
      "-----------------------------------------------Test: Epoch: 187| test loss: 0.706\n",
      "Train: Epoch: 188| train_loss: 0.571759\n",
      "-----------------------------------------------Test: Epoch: 188| test loss: 0.706\n",
      "Train: Epoch: 189| train_loss: 0.571672\n",
      "-----------------------------------------------Test: Epoch: 189| test loss: 0.706\n",
      "Train: Epoch: 190| train_loss: 0.571586\n",
      "-----------------------------------------------Test: Epoch: 190| test loss: 0.706\n",
      "Train: Epoch: 191| train_loss: 0.571500\n",
      "-----------------------------------------------Test: Epoch: 191| test loss: 0.706\n",
      "Train: Epoch: 192| train_loss: 0.571416\n",
      "-----------------------------------------------Test: Epoch: 192| test loss: 0.706\n",
      "Train: Epoch: 193| train_loss: 0.571332\n",
      "-----------------------------------------------Test: Epoch: 193| test loss: 0.706\n",
      "Train: Epoch: 194| train_loss: 0.571249\n",
      "-----------------------------------------------Test: Epoch: 194| test loss: 0.707\n",
      "Train: Epoch: 195| train_loss: 0.571167\n",
      "-----------------------------------------------Test: Epoch: 195| test loss: 0.707\n",
      "Train: Epoch: 196| train_loss: 0.571086\n",
      "-----------------------------------------------Test: Epoch: 196| test loss: 0.707\n",
      "Train: Epoch: 197| train_loss: 0.571005\n",
      "-----------------------------------------------Test: Epoch: 197| test loss: 0.707\n",
      "Train: Epoch: 198| train_loss: 0.570925\n",
      "-----------------------------------------------Test: Epoch: 198| test loss: 0.707\n",
      "Train: Epoch: 199| train_loss: 0.570846\n",
      "-----------------------------------------------Test: Epoch: 199| test loss: 0.707\n"
     ]
    }
   ],
   "source": [
    "tr_loss_tl = []\n",
    "te_loss_tl = []\n",
    "for e in range(200):\n",
    "    loss = loc_train(epoch=e, model=model, trainloader=loc_trainloader)\n",
    "    tr_loss_tl.append(loss)\n",
    "    teloss = test(epoch=e, model=model, testloader=loc_testloader)\n",
    "    te_loss_tl.append(teloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = list(range(1,len(tr_loss_tl)+1))\n",
    "plt.plot(x,tr_loss_tl, label='training loss')\n",
    "plt.plot(x,te_loss_tl, label='test loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('loc Training Monitoring')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model(loc_fea)\n",
    "loc_data['pre']=pre.detach().numpy().reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = lambda x: 0 if x<=0.5 else 1\n",
    "# loc_data[\"ori_cla\"]=loc_data.qPCR.apply(cls)\n",
    "loc_data[\"pre_cla\"]=loc_data.pre.apply(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6984126984126984\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "Y_te = loc_data[\"ori_cla\"].values.tolist()\n",
    "preds_te = loc_data[\"pre_cla\"].values.tolist()\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te))\n",
    "print('Recall:', recall_score(Y_te,preds_te))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5789473684210527\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "testpre = model(test_fea)\n",
    "test_data['pre'] = testpre.detach().numpy().reshape(1,-1)[0].tolist()\n",
    "cls = lambda x: 0 if x<=0.5 else 1\n",
    "test_data[\"pre_cla\"]=test_data.pre.apply(cls)\n",
    "\n",
    "# fill in your code...\n",
    "Y_te = test_data[\"ori_cla\"].values.tolist()\n",
    "preds_te = test_data[\"pre_cla\"].values.tolist()\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te))\n",
    "print('Recall:', recall_score(Y_te,preds_te))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
